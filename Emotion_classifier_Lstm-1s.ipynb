{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import phase if every import is here it is to allow you to run this cell and know if you have any issue and not after a few hours of training\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Activation, Flatten, MaxPooling1D, BatchNormalization, LSTM, GaussianNoise\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from src.VoiceModule import VoiceModule\n",
    "from src.classifiers.KerasClassifier import KerasClassifier\n",
    "from prepare_data_utils import load_wav, preprare_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fearful\n",
      "savee\n",
      "calm\n",
      "happy\n",
      "bdes\n",
      "surprised\n",
      "angry\n",
      "sad\n",
      "keywords\n"
     ]
    }
   ],
   "source": [
    "# print environment\n",
    "folder_path = \"../../data\"\n",
    "for l in os.listdir(folder_path):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data method\n",
    "\n",
    "def mfcc(buffer, samplerate, dim):\n",
    "    a = librosa.feature.mfcc(y=buffer, sr=samplerate, n_mfcc=dim)\n",
    "    a = np.transpose(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback to later draw some graphs\n",
    "\n",
    "class PerformanceLogger(Callback):\n",
    "    def __init__(self):\n",
    "        # List of tuple (loss, accuracy)\n",
    "        self.training = []\n",
    "        self.testing = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.training.append((logs[\"loss\"], logs[\"acc\"]))\n",
    "        self.testing.append((logs[\"val_loss\"], logs[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints metrics about a model\n",
    "def print_metrics(voice_module, X, Y, label_name_list):\n",
    "    Y_pred = voice_module.model._model.predict(X)\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    Y_max = np.argmax(Y, axis=1)\n",
    "    print(confusion_matrix(Y_max, Y_pred))\n",
    "    print(classification_report(Y_max, Y_pred, target_names=label_name_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class\n",
    "class EmotionClassifierLstm(KerasClassifier):\n",
    "    def __init__(self, file_path=None):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.expand_dims(np.array([x]), axis=2)\n",
    "        return self._model.predict(x)\n",
    "\n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        # This first layer add noises to the input data and serve as a data augmentation technique\n",
    "        # Used to prevent overfitting of the LSTM layer and try to extract more significant feature\n",
    "        model.add(GaussianNoise(0.4))\n",
    "        # This layer normalise the data to speed up the training and prevent the gradient of the LSTM to explode\n",
    "        # and reach exponential weight value\n",
    "        model.add(BatchNormalization())\n",
    "        # This is THE feature extraction layer\n",
    "        model.add(LSTM(64, input_shape=(35, 13)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        # This is the second part of the network, this one will be fine tuned later\n",
    "        model.add(GaussianNoise(0.2))\n",
    "\n",
    "        # The two last layers will be fine-tuned at the end of this notebook\n",
    "        model.add(Dense(32))\n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self._model = model\n",
    "        \n",
    "    def train(self, X, Y, batch_size=32, epoch=720, validation_data=None, callbacks=[]):\n",
    "        self._model.fit(X, Y, batch_size=batch_size, epochs=epoch, validation_data=validation_data, callbacks=callbacks)\n",
    "\n",
    "    def transform(self, x, samplerate):\n",
    "        to_process = mfcc(x, samplerate, 13)\n",
    "        to_process = pad_sequences([to_process], maxlen=35, padding='post')[0]\n",
    "        return to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciate model\n",
    "emotion_list = [\"calm\", \"happy\", \"angry\", \"fearful\", \"sad\"]\n",
    "dimension_mfcc = 13\n",
    "samplerate = 16000\n",
    "nb_break=None\n",
    "sample_duration=1\n",
    "step=0.5\n",
    "\n",
    "logger = PerformanceLogger()\n",
    "\n",
    "cls = EmotionClassifierLstm()\n",
    "vm = VoiceModule(\"emotion-1s\", emotion_list, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X, Y = preprare_wav(data, vm, sample_duration, step)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = f\"x_{'_'.join(emotion_list)}-{vm._name}.npy\"\n",
    "Y_file = f\"y_{'_'.join(emotion_list)}-{vm._name}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the prepared input data \n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for training\n",
    "X, Y = np.load(X_file), np.load(Y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584 1147\n",
      "(35, 13)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"train_size = 80 * len(X) // 100\n",
    "test_size = 20 * len(X) // 100\n",
    "#X = np.expand_dims(X, axis=3)\n",
    "X_train, X_test, = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]\"\"\"\n",
    "X = X.astype('float32')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "print(X[0].shape)\n",
    "print( X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4584 samples, validate on 1147 samples\n",
      "Epoch 1/100\n",
      "4584/4584 [==============================] - 6s 1ms/sample - loss: 1.5032 - acc: 0.3545 - val_loss: 1.5651 - val_acc: 0.2833\n",
      "Epoch 2/100\n",
      "4584/4584 [==============================] - 3s 699us/sample - loss: 1.3201 - acc: 0.4370 - val_loss: 1.5130 - val_acc: 0.3086\n",
      "Epoch 3/100\n",
      "4584/4584 [==============================] - 5s 1ms/sample - loss: 1.2481 - acc: 0.4686 - val_loss: 1.4342 - val_acc: 0.3862\n",
      "Epoch 4/100\n",
      "4584/4584 [==============================] - 4s 925us/sample - loss: 1.2023 - acc: 0.4889 - val_loss: 1.3407 - val_acc: 0.4342\n",
      "Epoch 5/100\n",
      "4584/4584 [==============================] - 3s 695us/sample - loss: 1.1448 - acc: 0.5212 - val_loss: 1.2521 - val_acc: 0.4804\n",
      "Epoch 6/100\n",
      "4584/4584 [==============================] - 4s 784us/sample - loss: 1.0937 - acc: 0.5414 - val_loss: 1.2066 - val_acc: 0.4900\n",
      "Epoch 7/100\n",
      "4584/4584 [==============================] - 3s 632us/sample - loss: 1.0447 - acc: 0.5705 - val_loss: 1.1439 - val_acc: 0.5179\n",
      "Epoch 8/100\n",
      "4584/4584 [==============================] - 4s 819us/sample - loss: 0.9956 - acc: 0.5818 - val_loss: 1.1087 - val_acc: 0.5397\n",
      "Epoch 9/100\n",
      "4584/4584 [==============================] - 3s 748us/sample - loss: 0.9563 - acc: 0.6156 - val_loss: 1.0944 - val_acc: 0.5571\n",
      "Epoch 10/100\n",
      "4584/4584 [==============================] - 3s 648us/sample - loss: 0.9079 - acc: 0.6298 - val_loss: 1.1114 - val_acc: 0.5597\n",
      "Epoch 11/100\n",
      "4584/4584 [==============================] - 3s 701us/sample - loss: 0.8647 - acc: 0.6527 - val_loss: 1.0999 - val_acc: 0.5702\n",
      "Epoch 12/100\n",
      "4584/4584 [==============================] - 3s 699us/sample - loss: 0.8292 - acc: 0.6769 - val_loss: 1.1277 - val_acc: 0.5693\n",
      "Epoch 13/100\n",
      "4584/4584 [==============================] - 4s 948us/sample - loss: 0.8081 - acc: 0.6760 - val_loss: 1.0512 - val_acc: 0.5867\n",
      "Epoch 14/100\n",
      "4584/4584 [==============================] - 4s 908us/sample - loss: 0.7693 - acc: 0.6885 - val_loss: 1.0825 - val_acc: 0.5946\n",
      "Epoch 15/100\n",
      "4584/4584 [==============================] - 4s 820us/sample - loss: 0.7505 - acc: 0.6957 - val_loss: 0.9903 - val_acc: 0.6138\n",
      "Epoch 16/100\n",
      "4584/4584 [==============================] - 4s 973us/sample - loss: 0.7061 - acc: 0.7192 - val_loss: 1.0117 - val_acc: 0.5937\n",
      "Epoch 17/100\n",
      "4584/4584 [==============================] - 4s 779us/sample - loss: 0.7258 - acc: 0.7112 - val_loss: 1.0836 - val_acc: 0.5824\n",
      "Epoch 18/100\n",
      "4584/4584 [==============================] - 3s 726us/sample - loss: 0.6786 - acc: 0.7312 - val_loss: 0.9726 - val_acc: 0.6295\n",
      "Epoch 19/100\n",
      "4584/4584 [==============================] - 4s 836us/sample - loss: 0.6462 - acc: 0.7439 - val_loss: 0.9775 - val_acc: 0.6277\n",
      "Epoch 20/100\n",
      "4584/4584 [==============================] - 4s 796us/sample - loss: 0.6339 - acc: 0.7507 - val_loss: 1.0075 - val_acc: 0.6330\n",
      "Epoch 21/100\n",
      "4584/4584 [==============================] - 4s 937us/sample - loss: 0.5947 - acc: 0.7653 - val_loss: 0.9530 - val_acc: 0.6495\n",
      "Epoch 22/100\n",
      "4584/4584 [==============================] - 5s 993us/sample - loss: 0.5955 - acc: 0.7627 - val_loss: 0.9839 - val_acc: 0.6530\n",
      "Epoch 23/100\n",
      "4584/4584 [==============================] - 3s 661us/sample - loss: 0.5721 - acc: 0.7773 - val_loss: 1.0043 - val_acc: 0.6321\n",
      "Epoch 24/100\n",
      "4584/4584 [==============================] - 3s 601us/sample - loss: 0.5747 - acc: 0.7768 - val_loss: 0.9039 - val_acc: 0.6757\n",
      "Epoch 25/100\n",
      "4584/4584 [==============================] - 4s 849us/sample - loss: 0.5366 - acc: 0.7840 - val_loss: 0.9607 - val_acc: 0.6469\n",
      "Epoch 26/100\n",
      "4584/4584 [==============================] - 4s 892us/sample - loss: 0.5125 - acc: 0.8024 - val_loss: 0.9643 - val_acc: 0.6591\n",
      "Epoch 27/100\n",
      "4584/4584 [==============================] - 4s 887us/sample - loss: 0.5206 - acc: 0.7995 - val_loss: 0.9464 - val_acc: 0.6748\n",
      "Epoch 28/100\n",
      "4584/4584 [==============================] - 4s 831us/sample - loss: 0.5071 - acc: 0.7928 - val_loss: 0.9890 - val_acc: 0.6504\n",
      "Epoch 29/100\n",
      "4584/4584 [==============================] - 4s 929us/sample - loss: 0.4863 - acc: 0.8172 - val_loss: 0.9655 - val_acc: 0.6582\n",
      "Epoch 30/100\n",
      "4584/4584 [==============================] - 4s 937us/sample - loss: 0.4637 - acc: 0.8120 - val_loss: 0.9767 - val_acc: 0.6722\n",
      "Epoch 31/100\n",
      "4584/4584 [==============================] - 4s 935us/sample - loss: 0.4627 - acc: 0.8220 - val_loss: 0.9997 - val_acc: 0.6626\n",
      "Epoch 32/100\n",
      "4584/4584 [==============================] - 4s 879us/sample - loss: 0.4428 - acc: 0.8272 - val_loss: 1.0095 - val_acc: 0.6591\n",
      "Epoch 33/100\n",
      "4584/4584 [==============================] - 4s 888us/sample - loss: 0.4047 - acc: 0.8416 - val_loss: 0.9751 - val_acc: 0.6818\n",
      "Epoch 34/100\n",
      "4584/4584 [==============================] - 4s 957us/sample - loss: 0.4299 - acc: 0.8294 - val_loss: 1.0238 - val_acc: 0.6861\n",
      "Epoch 35/100\n",
      "4584/4584 [==============================] - 4s 788us/sample - loss: 0.4204 - acc: 0.8397 - val_loss: 1.0314 - val_acc: 0.6687\n",
      "Epoch 36/100\n",
      "4584/4584 [==============================] - 4s 820us/sample - loss: 0.3991 - acc: 0.8462 - val_loss: 1.0911 - val_acc: 0.6765\n",
      "Epoch 37/100\n",
      "4584/4584 [==============================] - 4s 820us/sample - loss: 0.4139 - acc: 0.8462 - val_loss: 1.0897 - val_acc: 0.6495\n",
      "Epoch 38/100\n",
      "4584/4584 [==============================] - 4s 770us/sample - loss: 0.4085 - acc: 0.8370 - val_loss: 1.0866 - val_acc: 0.6556\n",
      "Epoch 39/100\n",
      "4584/4584 [==============================] - 4s 835us/sample - loss: 0.3850 - acc: 0.8497 - val_loss: 1.0432 - val_acc: 0.6731\n",
      "Epoch 40/100\n",
      "4584/4584 [==============================] - 4s 778us/sample - loss: 0.3743 - acc: 0.8543 - val_loss: 1.0504 - val_acc: 0.6966\n",
      "Epoch 41/100\n",
      "4584/4584 [==============================] - 4s 855us/sample - loss: 0.3510 - acc: 0.8637 - val_loss: 1.0418 - val_acc: 0.6879\n",
      "Epoch 42/100\n",
      "4584/4584 [==============================] - 4s 859us/sample - loss: 0.3661 - acc: 0.8647 - val_loss: 1.0256 - val_acc: 0.6783\n",
      "Epoch 43/100\n",
      "4584/4584 [==============================] - 4s 935us/sample - loss: 0.3387 - acc: 0.8713 - val_loss: 1.0318 - val_acc: 0.6800\n",
      "Epoch 44/100\n",
      "4584/4584 [==============================] - 4s 838us/sample - loss: 0.3203 - acc: 0.8794 - val_loss: 1.0743 - val_acc: 0.6809\n",
      "Epoch 45/100\n",
      "4584/4584 [==============================] - 4s 822us/sample - loss: 0.3139 - acc: 0.8824 - val_loss: 1.1854 - val_acc: 0.6434\n",
      "Epoch 46/100\n",
      "4584/4584 [==============================] - 4s 826us/sample - loss: 0.3098 - acc: 0.8842 - val_loss: 1.1568 - val_acc: 0.6774\n",
      "Epoch 47/100\n",
      "4584/4584 [==============================] - 4s 938us/sample - loss: 0.3313 - acc: 0.8737 - val_loss: 1.1254 - val_acc: 0.6827\n",
      "Epoch 48/100\n",
      "4584/4584 [==============================] - 4s 824us/sample - loss: 0.3302 - acc: 0.8726 - val_loss: 1.1951 - val_acc: 0.6652\n",
      "Epoch 49/100\n",
      "4584/4584 [==============================] - 4s 848us/sample - loss: 0.3066 - acc: 0.8848 - val_loss: 1.0220 - val_acc: 0.6905\n",
      "Epoch 50/100\n",
      "4584/4584 [==============================] - 4s 898us/sample - loss: 0.3014 - acc: 0.8846 - val_loss: 1.1000 - val_acc: 0.6827\n",
      "Epoch 51/100\n",
      "4584/4584 [==============================] - 4s 817us/sample - loss: 0.3052 - acc: 0.8826 - val_loss: 1.0797 - val_acc: 0.6905\n",
      "Epoch 52/100\n",
      "4584/4584 [==============================] - 4s 927us/sample - loss: 0.2869 - acc: 0.8916 - val_loss: 1.1048 - val_acc: 0.6905\n",
      "Epoch 53/100\n",
      "4584/4584 [==============================] - 4s 846us/sample - loss: 0.2835 - acc: 0.8918 - val_loss: 1.1140 - val_acc: 0.7001\n",
      "Epoch 54/100\n",
      "4584/4584 [==============================] - 4s 778us/sample - loss: 0.2756 - acc: 0.8959 - val_loss: 1.1186 - val_acc: 0.6983\n",
      "Epoch 55/100\n",
      "4584/4584 [==============================] - 3s 759us/sample - loss: 0.2598 - acc: 0.8981 - val_loss: 1.1584 - val_acc: 0.6931\n",
      "Epoch 56/100\n",
      "4584/4584 [==============================] - 4s 801us/sample - loss: 0.2829 - acc: 0.8903 - val_loss: 1.1418 - val_acc: 0.6914\n",
      "Epoch 57/100\n",
      "4584/4584 [==============================] - 4s 888us/sample - loss: 0.2647 - acc: 0.9040 - val_loss: 1.2174 - val_acc: 0.6870\n",
      "Epoch 58/100\n",
      "4584/4584 [==============================] - 4s 923us/sample - loss: 0.2425 - acc: 0.9073 - val_loss: 1.1724 - val_acc: 0.6861\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4584/4584 [==============================] - 4s 817us/sample - loss: 0.2503 - acc: 0.9047 - val_loss: 1.2293 - val_acc: 0.6774\n",
      "Epoch 60/100\n",
      "4584/4584 [==============================] - 4s 879us/sample - loss: 0.2547 - acc: 0.9071 - val_loss: 1.2058 - val_acc: 0.6870\n",
      "Epoch 61/100\n",
      "4584/4584 [==============================] - 4s 778us/sample - loss: 0.2545 - acc: 0.9029 - val_loss: 1.1484 - val_acc: 0.6888\n",
      "Epoch 62/100\n",
      "4584/4584 [==============================] - 4s 842us/sample - loss: 0.2296 - acc: 0.9116 - val_loss: 1.1970 - val_acc: 0.6800\n",
      "Epoch 63/100\n",
      "4584/4584 [==============================] - 5s 984us/sample - loss: 0.2458 - acc: 0.9058 - val_loss: 1.2864 - val_acc: 0.6774\n",
      "Epoch 64/100\n",
      "4584/4584 [==============================] - 4s 821us/sample - loss: 0.2867 - acc: 0.8949 - val_loss: 1.2258 - val_acc: 0.6861\n",
      "Epoch 65/100\n",
      "4584/4584 [==============================] - 4s 920us/sample - loss: 0.2186 - acc: 0.9186 - val_loss: 1.2429 - val_acc: 0.6931\n",
      "Epoch 66/100\n",
      "4584/4584 [==============================] - 4s 939us/sample - loss: 0.2166 - acc: 0.9184 - val_loss: 1.1769 - val_acc: 0.6844\n",
      "Epoch 67/100\n",
      "4584/4584 [==============================] - 4s 874us/sample - loss: 0.2218 - acc: 0.9171 - val_loss: 1.2074 - val_acc: 0.6975\n",
      "Epoch 68/100\n",
      "4584/4584 [==============================] - 3s 706us/sample - loss: 0.2193 - acc: 0.9197 - val_loss: 1.2584 - val_acc: 0.6861\n",
      "Epoch 69/100\n",
      "4584/4584 [==============================] - 4s 832us/sample - loss: 0.2157 - acc: 0.9217 - val_loss: 1.2008 - val_acc: 0.6949\n",
      "Epoch 70/100\n",
      "4584/4584 [==============================] - 4s 925us/sample - loss: 0.1992 - acc: 0.9267 - val_loss: 1.3244 - val_acc: 0.7036\n",
      "Epoch 71/100\n",
      "4584/4584 [==============================] - 4s 887us/sample - loss: 0.2258 - acc: 0.9162 - val_loss: 1.2947 - val_acc: 0.6879\n",
      "Epoch 72/100\n",
      "4584/4584 [==============================] - 5s 1ms/sample - loss: 0.2219 - acc: 0.9160 - val_loss: 1.2210 - val_acc: 0.7079\n",
      "Epoch 73/100\n",
      "4584/4584 [==============================] - 4s 899us/sample - loss: 0.2142 - acc: 0.9160 - val_loss: 1.2181 - val_acc: 0.7053\n",
      "Epoch 74/100\n",
      "4584/4584 [==============================] - 4s 921us/sample - loss: 0.2032 - acc: 0.9212 - val_loss: 1.3678 - val_acc: 0.6975\n",
      "Epoch 75/100\n",
      "4584/4584 [==============================] - 4s 853us/sample - loss: 0.2108 - acc: 0.9208 - val_loss: 1.3374 - val_acc: 0.6896\n",
      "Epoch 76/100\n",
      "4584/4584 [==============================] - 4s 795us/sample - loss: 0.2005 - acc: 0.9263 - val_loss: 1.3011 - val_acc: 0.6853\n",
      "Epoch 77/100\n",
      "4584/4584 [==============================] - 4s 825us/sample - loss: 0.1892 - acc: 0.9274 - val_loss: 1.2424 - val_acc: 0.6983\n",
      "Epoch 78/100\n",
      "4584/4584 [==============================] - 4s 840us/sample - loss: 0.1818 - acc: 0.9311 - val_loss: 1.2702 - val_acc: 0.6949\n",
      "Epoch 79/100\n",
      "4584/4584 [==============================] - 4s 886us/sample - loss: 0.1825 - acc: 0.9311 - val_loss: 1.3578 - val_acc: 0.6827\n",
      "Epoch 80/100\n",
      "4584/4584 [==============================] - 3s 763us/sample - loss: 0.1767 - acc: 0.9313 - val_loss: 1.3378 - val_acc: 0.7044\n",
      "Epoch 81/100\n",
      "4584/4584 [==============================] - 4s 880us/sample - loss: 0.1908 - acc: 0.9302 - val_loss: 1.2423 - val_acc: 0.7105\n",
      "Epoch 82/100\n",
      "4584/4584 [==============================] - 4s 867us/sample - loss: 0.1706 - acc: 0.9361 - val_loss: 1.3285 - val_acc: 0.6922\n",
      "Epoch 83/100\n",
      "4584/4584 [==============================] - 4s 922us/sample - loss: 0.1814 - acc: 0.9337 - val_loss: 1.3289 - val_acc: 0.6896\n",
      "Epoch 84/100\n",
      "4584/4584 [==============================] - 4s 941us/sample - loss: 0.1942 - acc: 0.9265 - val_loss: 1.3175 - val_acc: 0.6896\n",
      "Epoch 85/100\n",
      "4584/4584 [==============================] - 4s 968us/sample - loss: 0.2003 - acc: 0.9280 - val_loss: 1.2901 - val_acc: 0.7010\n",
      "Epoch 86/100\n",
      "4584/4584 [==============================] - 4s 882us/sample - loss: 0.1918 - acc: 0.9234 - val_loss: 1.3058 - val_acc: 0.6861\n",
      "Epoch 87/100\n",
      "4584/4584 [==============================] - 4s 846us/sample - loss: 0.1614 - acc: 0.9380 - val_loss: 1.2695 - val_acc: 0.7053\n",
      "Epoch 88/100\n",
      "4584/4584 [==============================] - 4s 952us/sample - loss: 0.1978 - acc: 0.9282 - val_loss: 1.2637 - val_acc: 0.7018\n",
      "Epoch 89/100\n",
      "4584/4584 [==============================] - 4s 837us/sample - loss: 0.1819 - acc: 0.9341 - val_loss: 1.2327 - val_acc: 0.6931\n",
      "Epoch 90/100\n",
      "4584/4584 [==============================] - 4s 783us/sample - loss: 0.1475 - acc: 0.9455 - val_loss: 1.2719 - val_acc: 0.7149\n",
      "Epoch 91/100\n",
      "4584/4584 [==============================] - 4s 772us/sample - loss: 0.1658 - acc: 0.9398 - val_loss: 1.2641 - val_acc: 0.7140\n",
      "Epoch 92/100\n",
      "4584/4584 [==============================] - 4s 816us/sample - loss: 0.1427 - acc: 0.9435 - val_loss: 1.3369 - val_acc: 0.7140\n",
      "Epoch 93/100\n",
      "4584/4584 [==============================] - 4s 883us/sample - loss: 0.1573 - acc: 0.9411 - val_loss: 1.3558 - val_acc: 0.6992\n",
      "Epoch 94/100\n",
      "4584/4584 [==============================] - 4s 937us/sample - loss: 0.1533 - acc: 0.9435 - val_loss: 1.4013 - val_acc: 0.6896\n",
      "Epoch 95/100\n",
      "4584/4584 [==============================] - 4s 921us/sample - loss: 0.1525 - acc: 0.9446 - val_loss: 1.4085 - val_acc: 0.6931\n",
      "Epoch 96/100\n",
      "4584/4584 [==============================] - 5s 990us/sample - loss: 0.1346 - acc: 0.9507 - val_loss: 1.3687 - val_acc: 0.6931\n",
      "Epoch 97/100\n",
      "4584/4584 [==============================] - 4s 830us/sample - loss: 0.1387 - acc: 0.9511 - val_loss: 1.3711 - val_acc: 0.6949\n",
      "Epoch 98/100\n",
      "4584/4584 [==============================] - 5s 1ms/sample - loss: 0.1502 - acc: 0.9420 - val_loss: 1.3587 - val_acc: 0.7105\n",
      "Epoch 99/100\n",
      "4584/4584 [==============================] - 4s 883us/sample - loss: 0.1780 - acc: 0.9396 - val_loss: 1.3935 - val_acc: 0.6914\n",
      "Epoch 100/100\n",
      "4584/4584 [==============================] - 4s 850us/sample - loss: 0.1587 - acc: 0.9428 - val_loss: 1.3385 - val_acc: 0.6905\n"
     ]
    }
   ],
   "source": [
    "vm.model.train(X_train, Y_train, batch_size=64, epoch=100, validation_data=(X_test, Y_test), callbacks=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise_2 (GaussianNo multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  52        \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  19968     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  256       \n",
      "_________________________________________________________________\n",
      "gaussian_noise_3 (GaussianNo multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  165       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 22,521\n",
      "Trainable params: 22,367\n",
      "Non-trainable params: 154\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f880418a940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[186  20   4  17  28]\n",
      " [ 10 157  13  24  12]\n",
      " [  6  28 158  28  13]\n",
      " [  7  24  12 151  18]\n",
      " [ 31  27  10  23 140]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.78      0.73      0.75       255\n",
      "       happy       0.61      0.73      0.67       216\n",
      "       angry       0.80      0.68      0.73       233\n",
      "     fearful       0.62      0.71      0.66       212\n",
      "         sad       0.66      0.61      0.63       231\n",
      "\n",
      "    accuracy                           0.69      1147\n",
      "   macro avg       0.70      0.69      0.69      1147\n",
      "weighted avg       0.70      0.69      0.69      1147\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm.model._model.summary()\n",
    "\n",
    "\n",
    "plt.plot(list(range(0, len(logger.training))), list(map(lambda x:x[1], logger.training)), label=\"training data\")\n",
    "plt.plot(list(range(0, len(logger.testing))), list(map(lambda x:x[1], logger.testing)), label=\"testing data\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "print_metrics(vm, X_test, Y_test, emotion_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see that the model largely overfit around the 20st epoch\n",
    "We can also see that the testing data's accuracy continues to grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we save the model so we don't have to retrain it each time\n",
    "vm.model.save(\"emotion_cnn-1s.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load the model to check if nothing went wrong\n",
    "vm.model.load(\"emotion_lstm-1s.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n"
     ]
    }
   ],
   "source": [
    "# Now we load the new dataset (savee)\n",
    "\n",
    "# First we change the folder path\n",
    "folder_path = \"../../data/savee\"\n",
    "\n",
    "# Now we load the data\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X_savee, Y_savee = preprare_wav(data, vm, sample_duration, step)\n",
    "X_savee = X_savee.astype('float32')\n",
    "X_savee_train, X_savee_test, Y_savee_train, Y_savee_test = train_test_split(X_savee, Y_savee, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1894, 35, 13)\n",
      "1894/1894 [==============================] - 1s 322us/sample - loss: 6.9077 - acc: 0.2371\n",
      "[[143  98  91   0  28]\n",
      " [ 82 153  71  36  26]\n",
      " [ 51 161  78  36  32]\n",
      " [ 77 119  93  29  40]\n",
      " [136 167  92   9  46]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.29      0.40      0.34       360\n",
      "       happy       0.22      0.42      0.29       368\n",
      "       angry       0.18      0.22      0.20       358\n",
      "     fearful       0.26      0.08      0.12       358\n",
      "         sad       0.27      0.10      0.15       450\n",
      "\n",
      "    accuracy                           0.24      1894\n",
      "   macro avg       0.25      0.24      0.22      1894\n",
      "weighted avg       0.25      0.24      0.22      1894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_savee.shape)\n",
    "vm.model._model.evaluate(X_savee, Y_savee)\n",
    "print_metrics(vm, X_savee, Y_savee, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "gaussian_noise_2\n",
      "batch_normalization_2\n",
      "lstm_1\n",
      "activation_2\n",
      "flatten_1\n",
      "batch_normalization_3\n",
      "gaussian_noise_3\n"
     ]
    }
   ],
   "source": [
    "# Now we freeze every layer used for extracting features from data\n",
    "savee_model = keras.models.clone_model(vm.model._model)\n",
    "for layer in savee_model.layers[:-3]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "savee_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "savee_cls = EmotionClassifierLstm()\n",
    "savee_cls._model = savee_model\n",
    "savee_vm = VoiceModule(\"emotion-1s\", emotion_list, savee_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1515 samples, validate on 379 samples\n",
      "Epoch 1/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.6418 - acc: 0.2686 - val_loss: 1.6205 - val_acc: 0.1557\n",
      "Epoch 2/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.4276 - acc: 0.3690 - val_loss: 1.6150 - val_acc: 0.1953\n",
      "Epoch 3/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3784 - acc: 0.4007 - val_loss: 1.6157 - val_acc: 0.1715\n",
      "Epoch 4/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3581 - acc: 0.3802 - val_loss: 1.6152 - val_acc: 0.2005\n",
      "Epoch 5/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3568 - acc: 0.3947 - val_loss: 1.6163 - val_acc: 0.1689\n",
      "Epoch 6/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3489 - acc: 0.4013 - val_loss: 1.6167 - val_acc: 0.1741\n",
      "Epoch 7/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3433 - acc: 0.4119 - val_loss: 1.6145 - val_acc: 0.2216\n",
      "Epoch 8/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3445 - acc: 0.3941 - val_loss: 1.6119 - val_acc: 0.2480\n",
      "Epoch 9/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3466 - acc: 0.3901 - val_loss: 1.6130 - val_acc: 0.2533\n",
      "Epoch 10/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3336 - acc: 0.4218 - val_loss: 1.6146 - val_acc: 0.2480\n",
      "Epoch 11/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3469 - acc: 0.4046 - val_loss: 1.6135 - val_acc: 0.2190\n",
      "Epoch 12/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3238 - acc: 0.4139 - val_loss: 1.6225 - val_acc: 0.2058\n",
      "Epoch 13/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3299 - acc: 0.4013 - val_loss: 1.6177 - val_acc: 0.2084\n",
      "Epoch 14/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3478 - acc: 0.4020 - val_loss: 1.6257 - val_acc: 0.2480\n",
      "Epoch 15/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3426 - acc: 0.4013 - val_loss: 1.6298 - val_acc: 0.2454\n",
      "Epoch 16/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3057 - acc: 0.4323 - val_loss: 1.6208 - val_acc: 0.2480\n",
      "Epoch 17/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3303 - acc: 0.4073 - val_loss: 1.6243 - val_acc: 0.2375\n",
      "Epoch 18/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3018 - acc: 0.4264 - val_loss: 1.6267 - val_acc: 0.2559\n",
      "Epoch 19/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3364 - acc: 0.4165 - val_loss: 1.6262 - val_acc: 0.2612\n",
      "Epoch 20/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3164 - acc: 0.4086 - val_loss: 1.6264 - val_acc: 0.2216\n",
      "Epoch 21/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3153 - acc: 0.4000 - val_loss: 1.6316 - val_acc: 0.2164\n",
      "Epoch 22/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2995 - acc: 0.4389 - val_loss: 1.6247 - val_acc: 0.2586\n",
      "Epoch 23/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3182 - acc: 0.4205 - val_loss: 1.6296 - val_acc: 0.2507\n",
      "Epoch 24/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3392 - acc: 0.4323 - val_loss: 1.6332 - val_acc: 0.2401\n",
      "Epoch 25/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3130 - acc: 0.4178 - val_loss: 1.6295 - val_acc: 0.2243\n",
      "Epoch 26/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3237 - acc: 0.4244 - val_loss: 1.6303 - val_acc: 0.2243\n",
      "Epoch 27/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3149 - acc: 0.4244 - val_loss: 1.6320 - val_acc: 0.2137\n",
      "Epoch 28/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3176 - acc: 0.4033 - val_loss: 1.6321 - val_acc: 0.2084\n",
      "Epoch 29/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3148 - acc: 0.4251 - val_loss: 1.6307 - val_acc: 0.2401\n",
      "Epoch 30/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3226 - acc: 0.4178 - val_loss: 1.6241 - val_acc: 0.2427\n",
      "Epoch 31/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.2856 - acc: 0.4495 - val_loss: 1.6294 - val_acc: 0.2032\n",
      "Epoch 32/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3036 - acc: 0.4337 - val_loss: 1.6311 - val_acc: 0.1794\n",
      "Epoch 33/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3041 - acc: 0.4284 - val_loss: 1.6326 - val_acc: 0.1953\n",
      "Epoch 34/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3112 - acc: 0.4297 - val_loss: 1.6308 - val_acc: 0.2243\n",
      "Epoch 35/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3025 - acc: 0.4271 - val_loss: 1.6275 - val_acc: 0.2480\n",
      "Epoch 36/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3319 - acc: 0.4172 - val_loss: 1.6332 - val_acc: 0.1926\n",
      "Epoch 37/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.2993 - acc: 0.4343 - val_loss: 1.6400 - val_acc: 0.1768\n",
      "Epoch 38/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.2919 - acc: 0.4251 - val_loss: 1.6418 - val_acc: 0.1689\n",
      "Epoch 39/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3161 - acc: 0.4132 - val_loss: 1.6382 - val_acc: 0.1821\n",
      "Epoch 40/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.2937 - acc: 0.4086 - val_loss: 1.6416 - val_acc: 0.1979\n",
      "Epoch 41/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3244 - acc: 0.4251 - val_loss: 1.6445 - val_acc: 0.1768\n",
      "Epoch 42/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3143 - acc: 0.4257 - val_loss: 1.6349 - val_acc: 0.1821\n",
      "Epoch 43/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3057 - acc: 0.4271 - val_loss: 1.6365 - val_acc: 0.1847\n",
      "Epoch 44/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3265 - acc: 0.4224 - val_loss: 1.6276 - val_acc: 0.1847\n",
      "Epoch 45/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3143 - acc: 0.4125 - val_loss: 1.6281 - val_acc: 0.1662\n",
      "Epoch 46/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3014 - acc: 0.4363 - val_loss: 1.6304 - val_acc: 0.1662\n",
      "Epoch 47/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3335 - acc: 0.4356 - val_loss: 1.6312 - val_acc: 0.1689\n",
      "Epoch 48/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2995 - acc: 0.4297 - val_loss: 1.6304 - val_acc: 0.1979\n",
      "Epoch 49/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3131 - acc: 0.4251 - val_loss: 1.6280 - val_acc: 0.2454\n",
      "Epoch 50/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3119 - acc: 0.4337 - val_loss: 1.6236 - val_acc: 0.2427\n",
      "Epoch 51/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3097 - acc: 0.4429 - val_loss: 1.6280 - val_acc: 0.2084\n",
      "Epoch 52/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3143 - acc: 0.4257 - val_loss: 1.6271 - val_acc: 0.2243\n",
      "Epoch 53/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3227 - acc: 0.4191 - val_loss: 1.6317 - val_acc: 0.2216\n",
      "Epoch 54/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3324 - acc: 0.4178 - val_loss: 1.6286 - val_acc: 0.2559\n",
      "Epoch 55/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3157 - acc: 0.4238 - val_loss: 1.6247 - val_acc: 0.2296\n",
      "Epoch 56/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3066 - acc: 0.4284 - val_loss: 1.6263 - val_acc: 0.2322\n",
      "Epoch 57/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3146 - acc: 0.4297 - val_loss: 1.6299 - val_acc: 0.2084\n",
      "Epoch 58/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2980 - acc: 0.4205 - val_loss: 1.6364 - val_acc: 0.1900\n",
      "Epoch 59/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3212 - acc: 0.4257 - val_loss: 1.6376 - val_acc: 0.1741\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3188 - acc: 0.4152 - val_loss: 1.6356 - val_acc: 0.1794\n",
      "Epoch 61/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3031 - acc: 0.4158 - val_loss: 1.6349 - val_acc: 0.1636\n",
      "Epoch 62/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2965 - acc: 0.4482 - val_loss: 1.6306 - val_acc: 0.1900\n",
      "Epoch 63/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3202 - acc: 0.4257 - val_loss: 1.6286 - val_acc: 0.1609\n",
      "Epoch 64/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3108 - acc: 0.4211 - val_loss: 1.6258 - val_acc: 0.1873\n",
      "Epoch 65/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.2940 - acc: 0.4330 - val_loss: 1.6296 - val_acc: 0.1741\n",
      "Epoch 66/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3258 - acc: 0.4119 - val_loss: 1.6340 - val_acc: 0.1662\n",
      "Epoch 67/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3410 - acc: 0.4119 - val_loss: 1.6305 - val_acc: 0.1847\n",
      "Epoch 68/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3272 - acc: 0.4218 - val_loss: 1.6285 - val_acc: 0.1926\n",
      "Epoch 69/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3130 - acc: 0.4119 - val_loss: 1.6321 - val_acc: 0.1741\n",
      "Epoch 70/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3008 - acc: 0.4330 - val_loss: 1.6313 - val_acc: 0.1821\n",
      "Epoch 71/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3078 - acc: 0.4370 - val_loss: 1.6439 - val_acc: 0.1609\n",
      "Epoch 72/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3049 - acc: 0.4264 - val_loss: 1.6357 - val_acc: 0.1873\n",
      "Epoch 73/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3136 - acc: 0.4185 - val_loss: 1.6403 - val_acc: 0.1530\n",
      "Epoch 74/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2973 - acc: 0.4363 - val_loss: 1.6352 - val_acc: 0.1636\n",
      "Epoch 75/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2985 - acc: 0.4238 - val_loss: 1.6366 - val_acc: 0.1662\n",
      "Epoch 76/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3006 - acc: 0.4343 - val_loss: 1.6329 - val_acc: 0.1847\n",
      "Epoch 77/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3212 - acc: 0.4198 - val_loss: 1.6310 - val_acc: 0.1715\n",
      "Epoch 78/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3157 - acc: 0.4191 - val_loss: 1.6349 - val_acc: 0.1478\n",
      "Epoch 79/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2937 - acc: 0.4409 - val_loss: 1.6379 - val_acc: 0.1504\n",
      "Epoch 80/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3193 - acc: 0.4257 - val_loss: 1.6408 - val_acc: 0.1583\n",
      "Epoch 81/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3110 - acc: 0.4284 - val_loss: 1.6402 - val_acc: 0.1530\n",
      "Epoch 82/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3062 - acc: 0.4323 - val_loss: 1.6396 - val_acc: 0.1662\n",
      "Epoch 83/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3017 - acc: 0.4251 - val_loss: 1.6363 - val_acc: 0.1530\n",
      "Epoch 84/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3106 - acc: 0.4304 - val_loss: 1.6380 - val_acc: 0.1504\n",
      "Epoch 85/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3122 - acc: 0.4271 - val_loss: 1.6389 - val_acc: 0.1530\n",
      "Epoch 86/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3031 - acc: 0.4442 - val_loss: 1.6395 - val_acc: 0.1609\n",
      "Epoch 87/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3178 - acc: 0.4284 - val_loss: 1.6361 - val_acc: 0.1662\n",
      "Epoch 88/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3096 - acc: 0.4297 - val_loss: 1.6297 - val_acc: 0.1636\n",
      "Epoch 89/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3125 - acc: 0.4396 - val_loss: 1.6261 - val_acc: 0.1821\n",
      "Epoch 90/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2859 - acc: 0.4455 - val_loss: 1.6246 - val_acc: 0.2084\n",
      "Epoch 91/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3072 - acc: 0.4310 - val_loss: 1.6198 - val_acc: 0.2375\n",
      "Epoch 92/100\n",
      "1515/1515 [==============================] - 2s 1ms/sample - loss: 1.3354 - acc: 0.4304 - val_loss: 1.6225 - val_acc: 0.2401\n",
      "Epoch 93/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3099 - acc: 0.4238 - val_loss: 1.6213 - val_acc: 0.2480\n",
      "Epoch 94/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.2993 - acc: 0.4198 - val_loss: 1.6245 - val_acc: 0.2269\n",
      "Epoch 95/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3025 - acc: 0.4356 - val_loss: 1.6233 - val_acc: 0.2032\n",
      "Epoch 96/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3158 - acc: 0.4323 - val_loss: 1.6260 - val_acc: 0.1979\n",
      "Epoch 97/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3037 - acc: 0.4363 - val_loss: 1.6286 - val_acc: 0.1847\n",
      "Epoch 98/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3246 - acc: 0.4251 - val_loss: 1.6296 - val_acc: 0.1662\n",
      "Epoch 99/100\n",
      "1515/1515 [==============================] - 2s 2ms/sample - loss: 1.3068 - acc: 0.4158 - val_loss: 1.6237 - val_acc: 0.1926\n",
      "Epoch 100/100\n",
      "1515/1515 [==============================] - 3s 2ms/sample - loss: 1.3138 - acc: 0.4317 - val_loss: 1.6229 - val_acc: 0.2137\n"
     ]
    }
   ],
   "source": [
    "# And we train the two last layers\n",
    "# This is classic fine tuning\n",
    "savee_vm.model.train(X_savee_train, Y_savee_train, batch_size=10, validation_data=(X_savee_test, Y_savee_test), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 26  9 37]\n",
      " [ 0  0 20  9 49]\n",
      " [ 0  0 16  7 42]\n",
      " [ 0  0 21 18 33]\n",
      " [ 0  0 34 11 47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.00      0.00      0.00        72\n",
      "       happy       0.00      0.00      0.00        78\n",
      "       angry       0.14      0.25      0.18        65\n",
      "     fearful       0.33      0.25      0.29        72\n",
      "         sad       0.23      0.51      0.31        92\n",
      "\n",
      "    accuracy                           0.21       379\n",
      "   macro avg       0.14      0.20      0.15       379\n",
      "weighted avg       0.14      0.21      0.16       379\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_metrics(savee_vm, X_savee_test, Y_savee_test, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n"
     ]
    }
   ],
   "source": [
    "# Now we load the new dataset (savee)\n",
    "\n",
    "# First we change the folder path\n",
    "folder_path = \"../../data/bdes\"\n",
    "\n",
    "# Now we load the data\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X_bdes, Y_bdes = preprare_wav(data, vm, sample_duration, step)\n",
    "X_bdes = X_bdes.astype('float32')\n",
    "X_bdes_train, X_bdes_test, Y_bdes_train, Y_bdes_test = train_test_split(X_bdes, Y_bdes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1448, 35, 13)\n",
      "1448/1448 [==============================] - 1s 791us/sample - loss: 7.1129 - acc: 0.2403\n",
      "[[  7  57 125  69   0]\n",
      " [  0  80  85  92   0]\n",
      " [  3  97 149  70   1]\n",
      " [  0  33  63 112   0]\n",
      " [ 28 128  76 173   0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.18      0.03      0.05       258\n",
      "       happy       0.20      0.31      0.25       257\n",
      "       angry       0.30      0.47      0.36       320\n",
      "     fearful       0.22      0.54      0.31       208\n",
      "         sad       0.00      0.00      0.00       405\n",
      "\n",
      "    accuracy                           0.24      1448\n",
      "   macro avg       0.18      0.27      0.19      1448\n",
      "weighted avg       0.17      0.24      0.18      1448\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(X_bdes.shape)\n",
    "vm.model._model.evaluate(X_bdes, Y_bdes)\n",
    "print_metrics(vm, X_bdes, Y_bdes, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_noise_2\n",
      "batch_normalization_2\n",
      "lstm_1\n",
      "activation_2\n",
      "flatten_1\n",
      "batch_normalization_3\n",
      "gaussian_noise_3\n"
     ]
    }
   ],
   "source": [
    "# Now we freeze every layer used for extracting features from data\n",
    "bdes_model = keras.models.clone_model(vm.model._model)\n",
    "for layer in bdes_model.layers[:-3]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "bdes_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bdes_cls = EmotionClassifierLstm()\n",
    "bdes_cls._model = bdes_model\n",
    "bdes_vm = VoiceModule(\"emotion-1s\", emotion_list, bdes_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1158 samples, validate on 290 samples\n",
      "Epoch 1/100\n",
      "1158/1158 [==============================] - 5s 4ms/sample - loss: 1.4354 - acc: 0.4076 - val_loss: 1.6884 - val_acc: 0.1966\n",
      "Epoch 2/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.1632 - acc: 0.5181 - val_loss: 1.6910 - val_acc: 0.2000\n",
      "Epoch 3/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.1127 - acc: 0.5225 - val_loss: 1.6869 - val_acc: 0.1931\n",
      "Epoch 4/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0649 - acc: 0.5535 - val_loss: 1.6817 - val_acc: 0.2000\n",
      "Epoch 5/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0460 - acc: 0.5423 - val_loss: 1.6787 - val_acc: 0.2138\n",
      "Epoch 6/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0454 - acc: 0.5648 - val_loss: 1.6749 - val_acc: 0.2034\n",
      "Epoch 7/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0140 - acc: 0.5674 - val_loss: 1.6598 - val_acc: 0.2172\n",
      "Epoch 8/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0092 - acc: 0.5630 - val_loss: 1.6637 - val_acc: 0.2103\n",
      "Epoch 9/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0099 - acc: 0.5648 - val_loss: 1.6801 - val_acc: 0.2103\n",
      "Epoch 10/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9869 - acc: 0.5855 - val_loss: 1.6626 - val_acc: 0.2069\n",
      "Epoch 11/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9884 - acc: 0.5803 - val_loss: 1.6731 - val_acc: 0.2207\n",
      "Epoch 12/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0036 - acc: 0.5881 - val_loss: 1.6663 - val_acc: 0.2034\n",
      "Epoch 13/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0022 - acc: 0.5820 - val_loss: 1.6792 - val_acc: 0.2310\n",
      "Epoch 14/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0051 - acc: 0.5777 - val_loss: 1.6841 - val_acc: 0.2414\n",
      "Epoch 15/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 1.0272 - acc: 0.5596 - val_loss: 1.6615 - val_acc: 0.2552\n",
      "Epoch 16/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0048 - acc: 0.5751 - val_loss: 1.6563 - val_acc: 0.2552\n",
      "Epoch 17/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9741 - acc: 0.5838 - val_loss: 1.6443 - val_acc: 0.2379\n",
      "Epoch 18/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9767 - acc: 0.5915 - val_loss: 1.6389 - val_acc: 0.2414\n",
      "Epoch 19/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9942 - acc: 0.5786 - val_loss: 1.6506 - val_acc: 0.2621\n",
      "Epoch 20/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9834 - acc: 0.5872 - val_loss: 1.6655 - val_acc: 0.2621\n",
      "Epoch 21/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9630 - acc: 0.5950 - val_loss: 1.6710 - val_acc: 0.2517\n",
      "Epoch 22/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9724 - acc: 0.5976 - val_loss: 1.6807 - val_acc: 0.2552\n",
      "Epoch 23/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 1.0003 - acc: 0.5803 - val_loss: 1.6700 - val_acc: 0.2655\n",
      "Epoch 24/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9963 - acc: 0.5881 - val_loss: 1.6746 - val_acc: 0.2448\n",
      "Epoch 25/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9754 - acc: 0.5907 - val_loss: 1.6719 - val_acc: 0.2724\n",
      "Epoch 26/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9818 - acc: 0.5907 - val_loss: 1.6659 - val_acc: 0.2724\n",
      "Epoch 27/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9865 - acc: 0.5794 - val_loss: 1.6605 - val_acc: 0.2483\n",
      "Epoch 28/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 1.0044 - acc: 0.5959 - val_loss: 1.6655 - val_acc: 0.2621\n",
      "Epoch 29/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9476 - acc: 0.5881 - val_loss: 1.6678 - val_acc: 0.2483\n",
      "Epoch 30/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9571 - acc: 0.5786 - val_loss: 1.6895 - val_acc: 0.2483\n",
      "Epoch 31/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9358 - acc: 0.5907 - val_loss: 1.6821 - val_acc: 0.2690\n",
      "Epoch 32/100\n",
      "1158/1158 [==============================] - 4s 4ms/sample - loss: 0.9749 - acc: 0.5803 - val_loss: 1.7102 - val_acc: 0.2483\n",
      "Epoch 33/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9691 - acc: 0.5812 - val_loss: 1.7295 - val_acc: 0.2379\n",
      "Epoch 34/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9887 - acc: 0.5725 - val_loss: 1.7224 - val_acc: 0.2586\n",
      "Epoch 35/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9735 - acc: 0.5846 - val_loss: 1.7263 - val_acc: 0.2655\n",
      "Epoch 36/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9960 - acc: 0.5889 - val_loss: 1.7105 - val_acc: 0.2621\n",
      "Epoch 37/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9844 - acc: 0.5933 - val_loss: 1.7219 - val_acc: 0.2414\n",
      "Epoch 38/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9544 - acc: 0.5924 - val_loss: 1.7389 - val_acc: 0.2276\n",
      "Epoch 39/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9749 - acc: 0.5855 - val_loss: 1.7428 - val_acc: 0.2276\n",
      "Epoch 40/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9901 - acc: 0.5864 - val_loss: 1.7391 - val_acc: 0.2448\n",
      "Epoch 41/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0099 - acc: 0.5682 - val_loss: 1.7332 - val_acc: 0.2241\n",
      "Epoch 42/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9414 - acc: 0.5976 - val_loss: 1.7366 - val_acc: 0.2448\n",
      "Epoch 43/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9606 - acc: 0.5786 - val_loss: 1.7365 - val_acc: 0.2345\n",
      "Epoch 44/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9892 - acc: 0.5751 - val_loss: 1.7321 - val_acc: 0.2517\n",
      "Epoch 45/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9667 - acc: 0.5846 - val_loss: 1.7250 - val_acc: 0.2448\n",
      "Epoch 46/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9904 - acc: 0.5708 - val_loss: 1.7286 - val_acc: 0.2345\n",
      "Epoch 47/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9566 - acc: 0.5907 - val_loss: 1.7312 - val_acc: 0.2483\n",
      "Epoch 48/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 1.0050 - acc: 0.5725 - val_loss: 1.7181 - val_acc: 0.2586\n",
      "Epoch 49/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9228 - acc: 0.6235 - val_loss: 1.7055 - val_acc: 0.2483\n",
      "Epoch 50/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9946 - acc: 0.5889 - val_loss: 1.7137 - val_acc: 0.2517\n",
      "Epoch 51/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9653 - acc: 0.5941 - val_loss: 1.7347 - val_acc: 0.2517\n",
      "Epoch 52/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9789 - acc: 0.5898 - val_loss: 1.7330 - val_acc: 0.2517\n",
      "Epoch 53/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9645 - acc: 0.5803 - val_loss: 1.7371 - val_acc: 0.2517\n",
      "Epoch 54/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9591 - acc: 0.6002 - val_loss: 1.7319 - val_acc: 0.2414\n",
      "Epoch 55/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0274 - acc: 0.5699 - val_loss: 1.7188 - val_acc: 0.2621\n",
      "Epoch 56/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9886 - acc: 0.5777 - val_loss: 1.7132 - val_acc: 0.2379\n",
      "Epoch 57/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9353 - acc: 0.5889 - val_loss: 1.7236 - val_acc: 0.2483\n",
      "Epoch 58/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9248 - acc: 0.6105 - val_loss: 1.7467 - val_acc: 0.2448\n",
      "Epoch 59/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9510 - acc: 0.5993 - val_loss: 1.7414 - val_acc: 0.2517\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9877 - acc: 0.5803 - val_loss: 1.7392 - val_acc: 0.2517\n",
      "Epoch 61/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 1.0251 - acc: 0.5708 - val_loss: 1.7289 - val_acc: 0.2517\n",
      "Epoch 62/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9919 - acc: 0.5786 - val_loss: 1.7301 - val_acc: 0.2448\n",
      "Epoch 63/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9434 - acc: 0.5941 - val_loss: 1.7305 - val_acc: 0.2586\n",
      "Epoch 64/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9892 - acc: 0.5682 - val_loss: 1.7400 - val_acc: 0.2483\n",
      "Epoch 65/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9574 - acc: 0.5838 - val_loss: 1.7394 - val_acc: 0.2586\n",
      "Epoch 66/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9631 - acc: 0.5984 - val_loss: 1.7376 - val_acc: 0.2414\n",
      "Epoch 67/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9527 - acc: 0.6166 - val_loss: 1.7446 - val_acc: 0.2414\n",
      "Epoch 68/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9813 - acc: 0.5812 - val_loss: 1.7139 - val_acc: 0.2655\n",
      "Epoch 69/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9785 - acc: 0.5872 - val_loss: 1.7146 - val_acc: 0.2552\n",
      "Epoch 70/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9463 - acc: 0.6149 - val_loss: 1.7216 - val_acc: 0.2655\n",
      "Epoch 71/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9621 - acc: 0.5829 - val_loss: 1.7170 - val_acc: 0.2621\n",
      "Epoch 72/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9835 - acc: 0.5743 - val_loss: 1.7240 - val_acc: 0.2448\n",
      "Epoch 73/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9646 - acc: 0.5950 - val_loss: 1.7329 - val_acc: 0.2552\n",
      "Epoch 74/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9603 - acc: 0.5820 - val_loss: 1.7178 - val_acc: 0.2483\n",
      "Epoch 75/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9641 - acc: 0.5881 - val_loss: 1.7176 - val_acc: 0.2552\n",
      "Epoch 76/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9950 - acc: 0.5889 - val_loss: 1.7310 - val_acc: 0.2483\n",
      "Epoch 77/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9853 - acc: 0.5674 - val_loss: 1.7298 - val_acc: 0.2448\n",
      "Epoch 78/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9438 - acc: 0.5976 - val_loss: 1.7363 - val_acc: 0.2414\n",
      "Epoch 79/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9612 - acc: 0.5967 - val_loss: 1.7427 - val_acc: 0.2414\n",
      "Epoch 80/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9725 - acc: 0.5941 - val_loss: 1.7362 - val_acc: 0.2310\n",
      "Epoch 81/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9635 - acc: 0.6054 - val_loss: 1.7208 - val_acc: 0.2517\n",
      "Epoch 82/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9843 - acc: 0.5915 - val_loss: 1.7227 - val_acc: 0.2310\n",
      "Epoch 83/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9685 - acc: 0.5898 - val_loss: 1.7203 - val_acc: 0.2345\n",
      "Epoch 84/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9208 - acc: 0.5984 - val_loss: 1.7165 - val_acc: 0.2379\n",
      "Epoch 85/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9718 - acc: 0.5855 - val_loss: 1.7258 - val_acc: 0.2310\n",
      "Epoch 86/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9962 - acc: 0.5881 - val_loss: 1.7261 - val_acc: 0.2379\n",
      "Epoch 87/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9375 - acc: 0.6036 - val_loss: 1.7362 - val_acc: 0.2310\n",
      "Epoch 88/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9573 - acc: 0.5855 - val_loss: 1.7228 - val_acc: 0.2414\n",
      "Epoch 89/100\n",
      "1158/1158 [==============================] - 4s 3ms/sample - loss: 0.9622 - acc: 0.5855 - val_loss: 1.7327 - val_acc: 0.2448\n",
      "Epoch 90/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9324 - acc: 0.6010 - val_loss: 1.7400 - val_acc: 0.2379\n",
      "Epoch 91/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9378 - acc: 0.5976 - val_loss: 1.7376 - val_acc: 0.2414\n",
      "Epoch 92/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9870 - acc: 0.5838 - val_loss: 1.7156 - val_acc: 0.2379\n",
      "Epoch 93/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9569 - acc: 0.5924 - val_loss: 1.7173 - val_acc: 0.2414\n",
      "Epoch 94/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9780 - acc: 0.5933 - val_loss: 1.7164 - val_acc: 0.2448\n",
      "Epoch 95/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9617 - acc: 0.6002 - val_loss: 1.7263 - val_acc: 0.2517\n",
      "Epoch 96/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9697 - acc: 0.5907 - val_loss: 1.7007 - val_acc: 0.2586\n",
      "Epoch 97/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9446 - acc: 0.5889 - val_loss: 1.6983 - val_acc: 0.2483\n",
      "Epoch 98/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9796 - acc: 0.5777 - val_loss: 1.6907 - val_acc: 0.2448\n",
      "Epoch 99/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 0.9538 - acc: 0.6002 - val_loss: 1.6965 - val_acc: 0.2483\n",
      "Epoch 100/100\n",
      "1158/1158 [==============================] - 3s 3ms/sample - loss: 1.0153 - acc: 0.5889 - val_loss: 1.6952 - val_acc: 0.2483\n"
     ]
    }
   ],
   "source": [
    "# And we train the two last layers\n",
    "# This is classic fine tuning\n",
    "bdes_vm.model.train(X_bdes_train, Y_bdes_train, batch_size=10, validation_data=(X_bdes_test, Y_bdes_test), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[35  5  6  0  0]\n",
      " [18  4 30  0  0]\n",
      " [17 10 33  0  0]\n",
      " [27  6 18  0  0]\n",
      " [64  5 12  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.22      0.76      0.34        46\n",
      "       happy       0.13      0.08      0.10        52\n",
      "       angry       0.33      0.55      0.42        60\n",
      "     fearful       0.00      0.00      0.00        51\n",
      "         sad       0.00      0.00      0.00        81\n",
      "\n",
      "    accuracy                           0.25       290\n",
      "   macro avg       0.14      0.28      0.17       290\n",
      "weighted avg       0.13      0.25      0.16       290\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_metrics(bdes_vm, X_bdes_test, Y_bdes_test, emotion_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
