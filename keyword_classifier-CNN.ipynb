{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import phase if every import is here it is to allow you to run this cell and know if you have any issue and not after a few hours of training\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Activation, Flatten, MaxPooling1D, BatchNormalization, LSTM, GaussianNoise\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "from src.VoiceModule import VoiceModule\n",
    "from src.classifiers.KerasClassifier import KerasClassifier\n",
    "from prepare_data_utils import load_wav, preprare_wav, split_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down\n",
      "tree\n",
      "_background_noise_\n",
      "go\n",
      "house\n",
      "validation_list.txt\n",
      "eight\n",
      "up\n",
      "bed\n",
      "two\n",
      "dog\n",
      "no\n",
      "bird\n",
      "five\n",
      "marvin\n",
      "seven\n",
      "four\n",
      "visual\n",
      "happy\n",
      "nine\n",
      "data_speech_commands_v0.02.tar.gz\n",
      "off\n",
      "yes\n",
      "forward\n",
      "follow\n",
      "README.md\n",
      "cat\n",
      "three\n",
      "on\n",
      "right\n",
      "backward\n",
      "testing_list.txt\n",
      "sheila\n",
      "wow\n",
      ".DS_Store\n",
      "stop\n",
      "one\n",
      "zero\n",
      "six\n",
      "learn\n",
      "LICENSE\n",
      "left\n"
     ]
    }
   ],
   "source": [
    "# print environment\n",
    "folder_path = \"../../data/keywords\"\n",
    "for l in os.listdir(folder_path):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data method\n",
    "\n",
    "def mfcc(buffer, samplerate, dim):\n",
    "    a = librosa.feature.mfcc(y=buffer, sr=samplerate, n_mfcc=dim)\n",
    "    a = np.transpose(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback to later draw some graphs\n",
    "\n",
    "class PerformanceLogger(Callback):\n",
    "    def __init__(self):\n",
    "        # List of tuple (loss, accuracy)\n",
    "        self.training = []\n",
    "        self.testing = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.training.append((logs[\"loss\"], logs[\"acc\"]))\n",
    "        self.testing.append((logs[\"val_loss\"], logs[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints metrics about a model\n",
    "def print_metrics(X, Y, label_name_list, voice_module):\n",
    "    Y_pred = voice_module.model._model.predict(X)\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    Y_max = np.argmax(Y, axis=1)\n",
    "    print(confusion_matrix(Y_max, Y_pred))\n",
    "    print(classification_report(Y_max, Y_pred, target_names=label_name_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class\n",
    "class KeywordClassifierCnn(KerasClassifier):\n",
    "    def __init__(self, file_path=None):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.expand_dims(np.array([x]), axis=2)\n",
    "        return self._model.predict(x)\n",
    "\n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        # Adding noise to the training data\n",
    "        model.add(GaussianNoise(0.2))\n",
    "        \n",
    "        # feature Extraction layers\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D(256, 3, input_shape=(50, 13), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(128, 3, input_shape=(50, 13), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(64, 3, input_shape=(50, 13), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(64, 3, input_shape=(50, 13), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        \n",
    "        # End of the neuron and classification part\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Dense(35))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self._model = model\n",
    "        \n",
    "    def train(self, X, Y, batch_size=32, epoch=720, validation_data=None, callbacks=[]):\n",
    "        self._model.fit(X, Y, batch_size=batch_size, epochs=epoch, validation_data=validation_data, callbacks=callbacks)\n",
    "\n",
    "    def transform(self, x, samplerate):\n",
    "        to_process = mfcc(x, samplerate, 13)\n",
    "        to_process = pad_sequences([to_process], maxlen=35, padding='post')[0]\n",
    "        return to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "#Instanciate model\n",
    "digit_list = [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\"]\n",
    "keyword_list = [\"backward\", \"bed\", \"bird\", \"cat\", \"dog\", \"down\", \"follow\", \"forward\",\n",
    "               \"go\", \"happy\", \"house\", \"learn\", \"left\", \"marvin\", \"no\", \"off\", \"on\",\n",
    "               \"right\", \"sheila\", \"stop\", \"tree\", \"up\", \"visual\", \"wow\", \"yes\"] + digit_list\n",
    "print(len(keyword_list))\n",
    "dimension_mfcc = 13\n",
    "samplerate = 16000\n",
    "nb_break=None\n",
    "sample_duration=1\n",
    "step=1\n",
    "\n",
    "logger = PerformanceLogger()\n",
    "\n",
    "cls = KeywordClassifierCnn()\n",
    "vm = VoiceModule(\"digit\", keyword_list, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing backward\n",
      "Doing bed\n",
      "Doing bird\n",
      "Doing cat\n",
      "Doing dog\n",
      "Doing down\n",
      "Doing follow\n",
      "Doing forward\n",
      "Doing go\n",
      "Doing happy\n",
      "Doing house\n",
      "Doing learn\n",
      "Doing left\n",
      "Doing marvin\n",
      "Doing no\n",
      "Doing off\n",
      "Doing on\n",
      "Doing right\n",
      "Doing sheila\n",
      "Doing stop\n",
      "Doing tree\n",
      "Doing up\n",
      "Doing visual\n",
      "Doing wow\n",
      "Doing yes\n",
      "Doing zero\n",
      "Doing one\n",
      "Doing two\n",
      "Doing three\n",
      "Doing four\n",
      "Doing five\n",
      "Doing six\n",
      "Doing seven\n",
      "Doing eight\n",
      "Doing nine\n",
      "Done (95394, 35, 13)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in keyword_list}\n",
    "X, Y = preprare_wav(data, vm, sample_duration, step)\n",
    "print(\"Done\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = f\"x_{'_'.join(keyword_list)}-{vm._name}.npy\"\n",
    "Y_file = f\"y_{'_'.join(keyword_list)}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the prepared input data \n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for training\n",
    "X, Y = np.load(X_file), np.load(Y_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95394, 35, 13)\n",
      "76315 19079\n",
      "(35, 13)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"train_size = 80 * len(X) // 100\n",
    "test_size = 20 * len(X) // 100\n",
    "#X = np.expand_dims(X, axis=3)\n",
    "X_train, X_test, = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]\"\"\"\n",
    "X = X.astype('float32')\n",
    "print(X.shape)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "print(X[0].shape)\n",
    "print( X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 76315 samples, validate on 19079 samples\n",
      "Epoch 1/100\n",
      "76315/76315 [==============================] - 11s 145us/sample - loss: 1.8688 - acc: 0.4722 - val_loss: 1.2662 - val_acc: 0.7357\n",
      "Epoch 2/100\n",
      "76315/76315 [==============================] - 9s 123us/sample - loss: 0.8338 - acc: 0.7518 - val_loss: 0.6829 - val_acc: 0.8169\n",
      "Epoch 3/100\n",
      "76315/76315 [==============================] - 9s 121us/sample - loss: 0.6234 - acc: 0.8145 - val_loss: 0.5317 - val_acc: 0.8426\n",
      "Epoch 4/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.5108 - acc: 0.8474 - val_loss: 0.3967 - val_acc: 0.8808\n",
      "Epoch 5/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.4423 - acc: 0.8678 - val_loss: 0.3389 - val_acc: 0.8996\n",
      "Epoch 6/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.3817 - acc: 0.8868 - val_loss: 0.3436 - val_acc: 0.8978\n",
      "Epoch 7/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.3592 - acc: 0.8921 - val_loss: 0.3535 - val_acc: 0.8970\n",
      "Epoch 8/100\n",
      "76315/76315 [==============================] - 9s 120us/sample - loss: 0.3202 - acc: 0.9044 - val_loss: 0.2720 - val_acc: 0.9193\n",
      "Epoch 9/100\n",
      "76315/76315 [==============================] - 9s 120us/sample - loss: 0.2845 - acc: 0.9148 - val_loss: 0.2938 - val_acc: 0.9135\n",
      "Epoch 10/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.2608 - acc: 0.9219 - val_loss: 0.2700 - val_acc: 0.9214\n",
      "Epoch 11/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.2386 - acc: 0.9278 - val_loss: 0.2734 - val_acc: 0.9237\n",
      "Epoch 12/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.2250 - acc: 0.9317 - val_loss: 0.2676 - val_acc: 0.9233\n",
      "Epoch 13/100\n",
      "76315/76315 [==============================] - 9s 122us/sample - loss: 0.2135 - acc: 0.9348 - val_loss: 0.2538 - val_acc: 0.9279\n",
      "Epoch 14/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.2037 - acc: 0.9379 - val_loss: 0.2521 - val_acc: 0.9296\n",
      "Epoch 15/100\n",
      "76315/76315 [==============================] - 9s 120us/sample - loss: 0.2145 - acc: 0.9346 - val_loss: 0.2455 - val_acc: 0.9303\n",
      "Epoch 16/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1833 - acc: 0.9445 - val_loss: 0.2477 - val_acc: 0.9287\n",
      "Epoch 17/100\n",
      "76315/76315 [==============================] - 9s 124us/sample - loss: 0.1827 - acc: 0.9435 - val_loss: 0.2506 - val_acc: 0.9311\n",
      "Epoch 18/100\n",
      "76315/76315 [==============================] - 9s 124us/sample - loss: 0.1726 - acc: 0.9467 - val_loss: 0.2378 - val_acc: 0.9335\n",
      "Epoch 19/100\n",
      "76315/76315 [==============================] - 9s 123us/sample - loss: 0.1421 - acc: 0.9565 - val_loss: 0.2453 - val_acc: 0.9353\n",
      "Epoch 20/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.1448 - acc: 0.9547 - val_loss: 0.2388 - val_acc: 0.9352\n",
      "Epoch 21/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.1475 - acc: 0.9542 - val_loss: 0.2489 - val_acc: 0.9339\n",
      "Epoch 22/100\n",
      "76315/76315 [==============================] - 9s 114us/sample - loss: 0.1364 - acc: 0.9574 - val_loss: 0.2503 - val_acc: 0.9327\n",
      "Epoch 23/100\n",
      "76315/76315 [==============================] - 9s 113us/sample - loss: 0.1397 - acc: 0.9570 - val_loss: 0.2373 - val_acc: 0.9358\n",
      "Epoch 24/100\n",
      "76315/76315 [==============================] - 9s 115us/sample - loss: 0.1181 - acc: 0.9632 - val_loss: 0.2354 - val_acc: 0.9382\n",
      "Epoch 25/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1138 - acc: 0.9640 - val_loss: 0.2466 - val_acc: 0.9394\n",
      "Epoch 26/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1044 - acc: 0.9674 - val_loss: 0.2677 - val_acc: 0.9313\n",
      "Epoch 27/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.1551 - acc: 0.9515 - val_loss: 0.2509 - val_acc: 0.9373\n",
      "Epoch 28/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1356 - acc: 0.9584 - val_loss: 0.2450 - val_acc: 0.9366\n",
      "Epoch 29/100\n",
      "76315/76315 [==============================] - 9s 115us/sample - loss: 0.1112 - acc: 0.9650 - val_loss: 0.2440 - val_acc: 0.9389\n",
      "Epoch 30/100\n",
      "76315/76315 [==============================] - 9s 114us/sample - loss: 0.0931 - acc: 0.9706 - val_loss: 0.2449 - val_acc: 0.9398\n",
      "Epoch 31/100\n",
      "76315/76315 [==============================] - 9s 114us/sample - loss: 0.0910 - acc: 0.9713 - val_loss: 0.2517 - val_acc: 0.9377\n",
      "Epoch 32/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.1065 - acc: 0.9668 - val_loss: 0.2669 - val_acc: 0.9347\n",
      "Epoch 33/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1091 - acc: 0.9661 - val_loss: 0.2433 - val_acc: 0.9409\n",
      "Epoch 34/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0919 - acc: 0.9709 - val_loss: 0.2629 - val_acc: 0.9369\n",
      "Epoch 35/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0871 - acc: 0.9723 - val_loss: 0.2626 - val_acc: 0.9372\n",
      "Epoch 36/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0911 - acc: 0.9705 - val_loss: 0.2583 - val_acc: 0.9405\n",
      "Epoch 37/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.1080 - acc: 0.9665 - val_loss: 0.2469 - val_acc: 0.9423\n",
      "Epoch 38/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0887 - acc: 0.9719 - val_loss: 0.2568 - val_acc: 0.9406\n",
      "Epoch 39/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0732 - acc: 0.9766 - val_loss: 0.2584 - val_acc: 0.9394\n",
      "Epoch 40/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0658 - acc: 0.9792 - val_loss: 0.2617 - val_acc: 0.9396\n",
      "Epoch 41/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.0653 - acc: 0.9795 - val_loss: 0.2633 - val_acc: 0.9402\n",
      "Epoch 42/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.0625 - acc: 0.9802 - val_loss: 0.2597 - val_acc: 0.9413\n",
      "Epoch 43/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0767 - acc: 0.9759 - val_loss: 0.2696 - val_acc: 0.9414\n",
      "Epoch 44/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0697 - acc: 0.9779 - val_loss: 0.2781 - val_acc: 0.9412\n",
      "Epoch 45/100\n",
      "76315/76315 [==============================] - 9s 115us/sample - loss: 0.0762 - acc: 0.9760 - val_loss: 0.3075 - val_acc: 0.9358\n",
      "Epoch 46/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1215 - acc: 0.9631 - val_loss: 0.2672 - val_acc: 0.9415\n",
      "Epoch 47/100\n",
      "76315/76315 [==============================] - 9s 115us/sample - loss: 0.0601 - acc: 0.9810 - val_loss: 0.2588 - val_acc: 0.9437\n",
      "Epoch 48/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0558 - acc: 0.9825 - val_loss: 0.2624 - val_acc: 0.9437\n",
      "Epoch 49/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0537 - acc: 0.9833 - val_loss: 0.2688 - val_acc: 0.9426\n",
      "Epoch 50/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0582 - acc: 0.9821 - val_loss: 0.2731 - val_acc: 0.9423\n",
      "Epoch 51/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0554 - acc: 0.9818 - val_loss: 0.2797 - val_acc: 0.9432\n",
      "Epoch 52/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0707 - acc: 0.9775 - val_loss: 0.3410 - val_acc: 0.9357\n",
      "Epoch 53/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.1115 - acc: 0.9674 - val_loss: 0.2869 - val_acc: 0.9400\n",
      "Epoch 54/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0786 - acc: 0.9755 - val_loss: 0.2709 - val_acc: 0.9423\n",
      "Epoch 55/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0517 - acc: 0.9833 - val_loss: 0.2745 - val_acc: 0.9433\n",
      "Epoch 56/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0499 - acc: 0.9842 - val_loss: 0.2714 - val_acc: 0.9440\n",
      "Epoch 57/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0568 - acc: 0.9820 - val_loss: 0.2816 - val_acc: 0.9409\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0535 - acc: 0.9829 - val_loss: 0.2705 - val_acc: 0.9438\n",
      "Epoch 59/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0476 - acc: 0.9854 - val_loss: 0.2769 - val_acc: 0.9450\n",
      "Epoch 60/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0426 - acc: 0.9866 - val_loss: 0.2835 - val_acc: 0.9438\n",
      "Epoch 61/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0420 - acc: 0.9865 - val_loss: 0.3144 - val_acc: 0.9393\n",
      "Epoch 62/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0520 - acc: 0.9836 - val_loss: 0.3118 - val_acc: 0.9416\n",
      "Epoch 63/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0940 - acc: 0.9720 - val_loss: 0.3014 - val_acc: 0.9393\n",
      "Epoch 64/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.1452 - acc: 0.9587 - val_loss: 0.2570 - val_acc: 0.9421\n",
      "Epoch 65/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0781 - acc: 0.9758 - val_loss: 0.2688 - val_acc: 0.9404\n",
      "Epoch 66/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0897 - acc: 0.9726 - val_loss: 0.2502 - val_acc: 0.9443\n",
      "Epoch 67/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0455 - acc: 0.9858 - val_loss: 0.2529 - val_acc: 0.9467\n",
      "Epoch 68/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0367 - acc: 0.9886 - val_loss: 0.2536 - val_acc: 0.9472\n",
      "Epoch 69/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0457 - acc: 0.9860 - val_loss: 0.2760 - val_acc: 0.9451\n",
      "Epoch 70/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0337 - acc: 0.9893 - val_loss: 0.2774 - val_acc: 0.9441\n",
      "Epoch 71/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0476 - acc: 0.9849 - val_loss: 0.2847 - val_acc: 0.9444\n",
      "Epoch 72/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0363 - acc: 0.9886 - val_loss: 0.2907 - val_acc: 0.9450\n",
      "Epoch 73/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.0591 - acc: 0.9816 - val_loss: 0.2992 - val_acc: 0.9433\n",
      "Epoch 74/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0478 - acc: 0.9851 - val_loss: 0.2950 - val_acc: 0.9429\n",
      "Epoch 75/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0446 - acc: 0.9860 - val_loss: 0.3013 - val_acc: 0.9430\n",
      "Epoch 76/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0319 - acc: 0.9899 - val_loss: 0.3149 - val_acc: 0.9420\n",
      "Epoch 77/100\n",
      "76315/76315 [==============================] - 9s 116us/sample - loss: 0.0638 - acc: 0.9807 - val_loss: 0.2990 - val_acc: 0.9428\n",
      "Epoch 78/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0496 - acc: 0.9842 - val_loss: 0.2955 - val_acc: 0.9443\n",
      "Epoch 79/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0441 - acc: 0.9861 - val_loss: 0.3002 - val_acc: 0.9442\n",
      "Epoch 80/100\n",
      "76315/76315 [==============================] - 9s 121us/sample - loss: 0.0305 - acc: 0.9905 - val_loss: 0.3098 - val_acc: 0.9465\n",
      "Epoch 81/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0378 - acc: 0.9889 - val_loss: 0.3051 - val_acc: 0.9439\n",
      "Epoch 82/100\n",
      "76315/76315 [==============================] - 9s 120us/sample - loss: 0.0530 - acc: 0.9839 - val_loss: 0.3076 - val_acc: 0.9444\n",
      "Epoch 83/100\n",
      "76315/76315 [==============================] - 8s 108us/sample - loss: 0.0825 - acc: 0.9757 - val_loss: 0.3095 - val_acc: 0.9415\n",
      "Epoch 84/100\n",
      "76315/76315 [==============================] - 8s 103us/sample - loss: 0.0633 - acc: 0.9808 - val_loss: 0.2893 - val_acc: 0.9445\n",
      "Epoch 85/100\n",
      "76315/76315 [==============================] - 8s 103us/sample - loss: 0.0349 - acc: 0.9890 - val_loss: 0.2817 - val_acc: 0.9448\n",
      "Epoch 86/100\n",
      "76315/76315 [==============================] - 8s 103us/sample - loss: 0.0361 - acc: 0.9889 - val_loss: 0.2990 - val_acc: 0.9461\n",
      "Epoch 87/100\n",
      "76315/76315 [==============================] - 8s 107us/sample - loss: 0.0274 - acc: 0.9917 - val_loss: 0.2940 - val_acc: 0.9480\n",
      "Epoch 88/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0261 - acc: 0.9923 - val_loss: 0.3229 - val_acc: 0.9450\n",
      "Epoch 89/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0449 - acc: 0.9858 - val_loss: 0.2989 - val_acc: 0.9460\n",
      "Epoch 90/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0271 - acc: 0.9916 - val_loss: 0.3123 - val_acc: 0.9445\n",
      "Epoch 91/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0740 - acc: 0.9780 - val_loss: 0.3026 - val_acc: 0.9433\n",
      "Epoch 92/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.1022 - acc: 0.9702 - val_loss: 0.2957 - val_acc: 0.9422\n",
      "Epoch 93/100\n",
      "76315/76315 [==============================] - 9s 120us/sample - loss: 0.0406 - acc: 0.9871 - val_loss: 0.2746 - val_acc: 0.9467\n",
      "Epoch 94/100\n",
      "76315/76315 [==============================] - 9s 117us/sample - loss: 0.0288 - acc: 0.9911 - val_loss: 0.2891 - val_acc: 0.9461\n",
      "Epoch 95/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0382 - acc: 0.9880 - val_loss: 0.3029 - val_acc: 0.9443\n",
      "Epoch 96/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0446 - acc: 0.9861 - val_loss: 0.2942 - val_acc: 0.9467\n",
      "Epoch 97/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0263 - acc: 0.9920 - val_loss: 0.2920 - val_acc: 0.9486\n",
      "Epoch 98/100\n",
      "76315/76315 [==============================] - 9s 119us/sample - loss: 0.0229 - acc: 0.9928 - val_loss: 0.3144 - val_acc: 0.9455\n",
      "Epoch 99/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0247 - acc: 0.9923 - val_loss: 0.3226 - val_acc: 0.9463\n",
      "Epoch 100/100\n",
      "76315/76315 [==============================] - 9s 118us/sample - loss: 0.0387 - acc: 0.9879 - val_loss: 0.3329 - val_acc: 0.9421\n"
     ]
    }
   ],
   "source": [
    "vm.model.train(X_train, Y_train, batch_size=512, epoch=100, validation_data=(X_test, Y_test), callbacks=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faa1c061668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[291   2   2 ...   1   0   0]\n",
      " [  1 321   8 ...   0   5   0]\n",
      " [  1  11 275 ...   2   0   2]\n",
      " ...\n",
      " [  1   2   2 ... 710   1   0]\n",
      " [  2   3   0 ...   2 668   1]\n",
      " [  0   2   2 ...   1   0 680]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    backward       0.95      0.95      0.95       307\n",
      "         bed       0.88      0.92      0.90       350\n",
      "        bird       0.86      0.90      0.88       307\n",
      "         cat       0.93      0.96      0.95       338\n",
      "         dog       0.90      0.92      0.91       367\n",
      "        down       0.93      0.94      0.93       729\n",
      "      follow       0.90      0.88      0.89       290\n",
      "     forward       0.91      0.89      0.90       281\n",
      "          go       0.86      0.96      0.90       670\n",
      "       happy       0.98      0.95      0.96       362\n",
      "       house       0.99      0.94      0.96       377\n",
      "       learn       0.93      0.85      0.89       281\n",
      "        left       0.95      0.94      0.94       667\n",
      "      marvin       0.95      0.92      0.94       370\n",
      "          no       0.95      0.87      0.91       703\n",
      "         off       0.94      0.95      0.95       710\n",
      "          on       0.92      0.95      0.93       729\n",
      "       right       0.95      0.97      0.96       688\n",
      "      sheila       0.98      0.95      0.97       364\n",
      "        stop       0.97      0.97      0.97       733\n",
      "        tree       0.89      0.86      0.88       273\n",
      "          up       0.94      0.94      0.94       672\n",
      "      visual       0.99      0.93      0.96       295\n",
      "         wow       0.94      0.93      0.93       374\n",
      "         yes       0.98      0.98      0.98       714\n",
      "        zero       0.97      0.98      0.97       743\n",
      "         one       0.96      0.94      0.95       724\n",
      "         two       0.95      0.95      0.95       699\n",
      "       three       0.94      0.91      0.93       727\n",
      "        four       0.91      0.94      0.92       707\n",
      "        five       0.94      0.96      0.95       698\n",
      "         six       0.98      0.97      0.97       686\n",
      "       seven       0.95      0.96      0.96       736\n",
      "       eight       0.97      0.96      0.96       696\n",
      "        nine       0.97      0.96      0.96       712\n",
      "\n",
      "    accuracy                           0.94     19079\n",
      "   macro avg       0.94      0.94      0.94     19079\n",
      "weighted avg       0.94      0.94      0.94     19079\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(list(range(0, len(logger.training))), list(map(lambda x:x[1], logger.training)), label=\"training data\")\n",
    "plt.plot(list(range(0, len(logger.testing))), list(map(lambda x:x[1], logger.testing)), label=\"testing data\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "print_metrics(X_test, Y_test, keyword_list, vm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm.model.save(\"keyword-cnn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class\n",
    "class EmotionClassifier(KerasClassifier):\n",
    "    def __init__(self, file_path=None):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.expand_dims(np.array([x]), axis=2)\n",
    "        return self._model.predict(x)\n",
    "\n",
    "    def build(self):\n",
    "        # Now we freeze every layer used for extracting features from data\n",
    "        fine_tuned_model = Sequential(vm.model._model.layers[:-3])\n",
    "        fine_tuned_model.add(Dense(128))\n",
    "        fine_tuned_model.add(Dense(6))\n",
    "        fine_tuned_model.add(Activation('softmax'))\n",
    "        fine_tuned_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # Here I freeze the weoghts for all layer exept the two last one\n",
    "        for layer in fine_tuned_model.layers[:-9]:\n",
    "            print(layer.name)\n",
    "            layer.trainable = False\n",
    "        fine_tuned_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])        \n",
    "        self._model = fine_tuned_model\n",
    "\n",
    "    def train(self, X, Y, batch_size=32, epoch=720, validation_data=None, callbacks=[]):\n",
    "        self._model.fit(X, Y, batch_size=batch_size, epochs=epoch, validation_data=validation_data, callbacks=callbacks)\n",
    "\n",
    "    def transform(self, x, samplerate):\n",
    "        to_process = mfcc(x, samplerate, 13)\n",
    "        to_process = pad_sequences([to_process], maxlen=35, padding='post')[0]\n",
    "        return to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_noise_6\n",
      "batch_normalization_12\n",
      "conv1d_24\n",
      "activation_36\n",
      "max_pooling1d_24\n",
      "conv1d_25\n",
      "activation_37\n",
      "max_pooling1d_25\n",
      "conv1d_26\n",
      "activation_38\n",
      "max_pooling1d_26\n"
     ]
    }
   ],
   "source": [
    "# Now we load the new dataset (savee)\n",
    "emotion_list = [\"calm\", \"happy\", \"angry\", \"fearful\", \"surprised\", \"sad\"]\n",
    "emotion_vm = VoiceModule(\"emotion\", emotion_list, EmotionClassifier())\n",
    "# First we change the folder path\n",
    "folder_path = \"../../data/savee\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing surprised\n",
      "Doing sad\n"
     ]
    }
   ],
   "source": [
    "# Now we load the data\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X_savee, Y_savee = preprare_wav(data, emotion_vm, sample_duration, step)\n",
    "X_savee = X_savee.astype('float32')\n",
    "X_savee_train, X_savee_test, Y_savee_train, Y_savee_test = train_test_split(X_savee, Y_savee, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1119 samples, validate on 280 samples\n",
      "Epoch 1/100\n",
      "1119/1119 [==============================] - 1s 1ms/sample - loss: 2.1787 - acc: 0.2029 - val_loss: 1.8436 - val_acc: 0.1250\n",
      "Epoch 2/100\n",
      "1119/1119 [==============================] - 1s 693us/sample - loss: 1.9041 - acc: 0.2288 - val_loss: 1.8036 - val_acc: 0.2036\n",
      "Epoch 3/100\n",
      "1119/1119 [==============================] - 1s 651us/sample - loss: 1.8367 - acc: 0.2502 - val_loss: 1.7885 - val_acc: 0.2000\n",
      "Epoch 4/100\n",
      "1119/1119 [==============================] - 1s 674us/sample - loss: 1.8167 - acc: 0.2717 - val_loss: 1.7902 - val_acc: 0.1821\n",
      "Epoch 5/100\n",
      "1119/1119 [==============================] - 1s 605us/sample - loss: 1.8024 - acc: 0.2601 - val_loss: 1.8016 - val_acc: 0.2000\n",
      "Epoch 6/100\n",
      "1119/1119 [==============================] - 1s 617us/sample - loss: 1.8203 - acc: 0.2556 - val_loss: 1.8007 - val_acc: 0.2071\n",
      "Epoch 7/100\n",
      "1119/1119 [==============================] - 1s 635us/sample - loss: 1.8054 - acc: 0.2556 - val_loss: 1.7966 - val_acc: 0.2357\n",
      "Epoch 8/100\n",
      "1119/1119 [==============================] - 1s 623us/sample - loss: 1.7800 - acc: 0.2601 - val_loss: 1.8023 - val_acc: 0.2357\n",
      "Epoch 9/100\n",
      "1119/1119 [==============================] - 1s 616us/sample - loss: 1.8126 - acc: 0.2431 - val_loss: 1.7839 - val_acc: 0.2143\n",
      "Epoch 10/100\n",
      "1119/1119 [==============================] - 1s 624us/sample - loss: 1.7975 - acc: 0.2556 - val_loss: 1.7775 - val_acc: 0.2286\n",
      "Epoch 11/100\n",
      "1119/1119 [==============================] - 1s 632us/sample - loss: 1.8067 - acc: 0.2618 - val_loss: 1.8046 - val_acc: 0.2036\n",
      "Epoch 12/100\n",
      "1119/1119 [==============================] - 1s 630us/sample - loss: 1.7753 - acc: 0.2592 - val_loss: 1.7716 - val_acc: 0.2571\n",
      "Epoch 13/100\n",
      "1119/1119 [==============================] - 1s 595us/sample - loss: 1.7913 - acc: 0.2645 - val_loss: 1.7842 - val_acc: 0.2250\n",
      "Epoch 14/100\n",
      "1119/1119 [==============================] - 1s 691us/sample - loss: 1.7661 - acc: 0.2609 - val_loss: 1.7811 - val_acc: 0.2143\n",
      "Epoch 15/100\n",
      "1119/1119 [==============================] - 1s 643us/sample - loss: 1.7583 - acc: 0.2690 - val_loss: 1.7736 - val_acc: 0.2179\n",
      "Epoch 16/100\n",
      "1119/1119 [==============================] - 1s 619us/sample - loss: 1.7828 - acc: 0.2636 - val_loss: 1.7832 - val_acc: 0.2107\n",
      "Epoch 17/100\n",
      "1119/1119 [==============================] - 1s 651us/sample - loss: 1.7570 - acc: 0.2699 - val_loss: 1.7819 - val_acc: 0.1929\n",
      "Epoch 18/100\n",
      "1119/1119 [==============================] - 1s 676us/sample - loss: 1.7494 - acc: 0.2681 - val_loss: 1.7798 - val_acc: 0.2250\n",
      "Epoch 19/100\n",
      "1119/1119 [==============================] - 1s 947us/sample - loss: 1.7592 - acc: 0.2654 - val_loss: 1.7948 - val_acc: 0.1536\n",
      "Epoch 20/100\n",
      "1119/1119 [==============================] - 1s 954us/sample - loss: 1.7373 - acc: 0.2958 - val_loss: 1.7754 - val_acc: 0.2036\n",
      "Epoch 21/100\n",
      "1119/1119 [==============================] - 1s 958us/sample - loss: 1.7242 - acc: 0.2967 - val_loss: 1.7732 - val_acc: 0.2214\n",
      "Epoch 22/100\n",
      "1119/1119 [==============================] - 1s 935us/sample - loss: 1.7692 - acc: 0.2431 - val_loss: 1.7776 - val_acc: 0.2500\n",
      "Epoch 23/100\n",
      "1119/1119 [==============================] - 1s 969us/sample - loss: 1.7607 - acc: 0.2556 - val_loss: 1.7789 - val_acc: 0.2429\n",
      "Epoch 24/100\n",
      "1119/1119 [==============================] - 1s 975us/sample - loss: 1.7432 - acc: 0.2699 - val_loss: 1.7751 - val_acc: 0.2357\n",
      "Epoch 25/100\n",
      "1119/1119 [==============================] - 1s 989us/sample - loss: 1.7077 - acc: 0.2931 - val_loss: 1.7780 - val_acc: 0.2464\n",
      "Epoch 26/100\n",
      "1119/1119 [==============================] - 1s 986us/sample - loss: 1.7520 - acc: 0.2502 - val_loss: 1.7615 - val_acc: 0.2250\n",
      "Epoch 27/100\n",
      "1119/1119 [==============================] - 1s 970us/sample - loss: 1.7362 - acc: 0.2824 - val_loss: 1.7753 - val_acc: 0.1964\n",
      "Epoch 28/100\n",
      "1119/1119 [==============================] - 1s 984us/sample - loss: 1.7233 - acc: 0.2878 - val_loss: 1.7762 - val_acc: 0.2179\n",
      "Epoch 29/100\n",
      "1119/1119 [==============================] - 1s 972us/sample - loss: 1.7454 - acc: 0.2681 - val_loss: 1.7765 - val_acc: 0.2393\n",
      "Epoch 30/100\n",
      "1119/1119 [==============================] - 1s 955us/sample - loss: 1.7186 - acc: 0.2797 - val_loss: 1.7743 - val_acc: 0.2000\n",
      "Epoch 31/100\n",
      "1119/1119 [==============================] - 1s 966us/sample - loss: 1.7024 - acc: 0.2913 - val_loss: 1.7745 - val_acc: 0.2179\n",
      "Epoch 32/100\n",
      "1119/1119 [==============================] - 1s 972us/sample - loss: 1.7204 - acc: 0.2931 - val_loss: 1.7726 - val_acc: 0.1964\n",
      "Epoch 33/100\n",
      "1119/1119 [==============================] - 1s 960us/sample - loss: 1.7246 - acc: 0.2574 - val_loss: 1.7678 - val_acc: 0.2143\n",
      "Epoch 34/100\n",
      "1119/1119 [==============================] - 1s 966us/sample - loss: 1.7175 - acc: 0.2752 - val_loss: 1.7726 - val_acc: 0.2179\n",
      "Epoch 35/100\n",
      "1119/1119 [==============================] - 1s 982us/sample - loss: 1.7175 - acc: 0.2663 - val_loss: 1.7716 - val_acc: 0.2286\n",
      "Epoch 36/100\n",
      "1119/1119 [==============================] - 1s 960us/sample - loss: 1.7285 - acc: 0.2869 - val_loss: 1.7630 - val_acc: 0.2107\n",
      "Epoch 37/100\n",
      "1119/1119 [==============================] - 1s 956us/sample - loss: 1.6951 - acc: 0.3083 - val_loss: 1.7694 - val_acc: 0.2071\n",
      "Epoch 38/100\n",
      "1119/1119 [==============================] - 1s 958us/sample - loss: 1.7035 - acc: 0.2761 - val_loss: 1.7773 - val_acc: 0.2036\n",
      "Epoch 39/100\n",
      "1119/1119 [==============================] - 1s 971us/sample - loss: 1.6978 - acc: 0.2833 - val_loss: 1.7723 - val_acc: 0.2143\n",
      "Epoch 40/100\n",
      "1119/1119 [==============================] - 1s 960us/sample - loss: 1.7366 - acc: 0.2797 - val_loss: 1.7735 - val_acc: 0.2071\n",
      "Epoch 41/100\n",
      "1119/1119 [==============================] - 1s 960us/sample - loss: 1.7247 - acc: 0.2770 - val_loss: 1.7783 - val_acc: 0.2250\n",
      "Epoch 42/100\n",
      "1119/1119 [==============================] - 1s 980us/sample - loss: 1.7093 - acc: 0.3012 - val_loss: 1.7777 - val_acc: 0.2429\n",
      "Epoch 43/100\n",
      "1119/1119 [==============================] - 1s 977us/sample - loss: 1.7109 - acc: 0.2922 - val_loss: 1.7776 - val_acc: 0.2321\n",
      "Epoch 44/100\n",
      "1119/1119 [==============================] - 1s 965us/sample - loss: 1.6956 - acc: 0.2878 - val_loss: 1.7748 - val_acc: 0.2536\n",
      "Epoch 45/100\n",
      "1119/1119 [==============================] - 1s 964us/sample - loss: 1.7192 - acc: 0.3003 - val_loss: 1.7732 - val_acc: 0.2321\n",
      "Epoch 46/100\n",
      "1119/1119 [==============================] - 1s 968us/sample - loss: 1.7039 - acc: 0.2887 - val_loss: 1.7806 - val_acc: 0.2214\n",
      "Epoch 47/100\n",
      "1119/1119 [==============================] - 1s 957us/sample - loss: 1.7020 - acc: 0.2949 - val_loss: 1.7746 - val_acc: 0.2357\n",
      "Epoch 48/100\n",
      "1119/1119 [==============================] - 1s 946us/sample - loss: 1.7008 - acc: 0.2744 - val_loss: 1.7708 - val_acc: 0.2321\n",
      "Epoch 49/100\n",
      "1119/1119 [==============================] - 1s 978us/sample - loss: 1.6831 - acc: 0.3128 - val_loss: 1.7719 - val_acc: 0.2500\n",
      "Epoch 50/100\n",
      "1119/1119 [==============================] - 1s 973us/sample - loss: 1.7087 - acc: 0.2976 - val_loss: 1.7720 - val_acc: 0.2464\n",
      "Epoch 51/100\n",
      "1119/1119 [==============================] - 1s 957us/sample - loss: 1.7141 - acc: 0.2806 - val_loss: 1.7763 - val_acc: 0.2250\n",
      "Epoch 52/100\n",
      "1119/1119 [==============================] - 1s 978us/sample - loss: 1.7169 - acc: 0.2770 - val_loss: 1.7737 - val_acc: 0.2500\n",
      "Epoch 53/100\n",
      "1119/1119 [==============================] - 1s 976us/sample - loss: 1.7023 - acc: 0.2878 - val_loss: 1.7818 - val_acc: 0.2179\n",
      "Epoch 54/100\n",
      "1119/1119 [==============================] - 1s 953us/sample - loss: 1.6999 - acc: 0.2770 - val_loss: 1.7809 - val_acc: 0.2214\n",
      "Epoch 55/100\n",
      "1119/1119 [==============================] - 1s 952us/sample - loss: 1.6964 - acc: 0.3038 - val_loss: 1.7739 - val_acc: 0.2107\n",
      "Epoch 56/100\n",
      "1119/1119 [==============================] - 1s 977us/sample - loss: 1.7080 - acc: 0.2842 - val_loss: 1.7742 - val_acc: 0.2357\n",
      "Epoch 57/100\n",
      "1119/1119 [==============================] - 1s 973us/sample - loss: 1.7122 - acc: 0.2851 - val_loss: 1.7757 - val_acc: 0.2429\n",
      "Epoch 58/100\n",
      "1119/1119 [==============================] - 1s 969us/sample - loss: 1.6935 - acc: 0.2931 - val_loss: 1.7794 - val_acc: 0.2286\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1119/1119 [==============================] - 1s 949us/sample - loss: 1.6891 - acc: 0.2806 - val_loss: 1.7859 - val_acc: 0.2321\n",
      "Epoch 60/100\n",
      "1119/1119 [==============================] - 1s 900us/sample - loss: 1.6925 - acc: 0.2985 - val_loss: 1.7780 - val_acc: 0.2214\n",
      "Epoch 61/100\n",
      "1119/1119 [==============================] - 1s 798us/sample - loss: 1.6973 - acc: 0.2860 - val_loss: 1.7807 - val_acc: 0.2393\n",
      "Epoch 62/100\n",
      "1119/1119 [==============================] - 1s 789us/sample - loss: 1.7023 - acc: 0.2842 - val_loss: 1.7788 - val_acc: 0.2214\n",
      "Epoch 63/100\n",
      "1119/1119 [==============================] - 1s 796us/sample - loss: 1.7058 - acc: 0.2904 - val_loss: 1.7697 - val_acc: 0.2214\n",
      "Epoch 64/100\n",
      "1119/1119 [==============================] - 1s 832us/sample - loss: 1.6910 - acc: 0.3074 - val_loss: 1.7790 - val_acc: 0.2107\n",
      "Epoch 65/100\n",
      "1119/1119 [==============================] - 1s 880us/sample - loss: 1.7119 - acc: 0.2779 - val_loss: 1.7741 - val_acc: 0.2250\n",
      "Epoch 66/100\n",
      "1119/1119 [==============================] - 1s 879us/sample - loss: 1.6921 - acc: 0.2895 - val_loss: 1.7708 - val_acc: 0.2179\n",
      "Epoch 67/100\n",
      "1119/1119 [==============================] - 1s 836us/sample - loss: 1.7021 - acc: 0.2940 - val_loss: 1.7680 - val_acc: 0.2393\n",
      "Epoch 68/100\n",
      "1119/1119 [==============================] - 1s 875us/sample - loss: 1.7011 - acc: 0.2931 - val_loss: 1.7663 - val_acc: 0.2357\n",
      "Epoch 69/100\n",
      "1119/1119 [==============================] - 1s 901us/sample - loss: 1.6935 - acc: 0.3137 - val_loss: 1.7659 - val_acc: 0.2500\n",
      "Epoch 70/100\n",
      "1119/1119 [==============================] - 1s 936us/sample - loss: 1.7070 - acc: 0.2770 - val_loss: 1.7658 - val_acc: 0.2714\n",
      "Epoch 71/100\n",
      "1119/1119 [==============================] - 1s 935us/sample - loss: 1.6758 - acc: 0.3137 - val_loss: 1.7664 - val_acc: 0.2357\n",
      "Epoch 72/100\n",
      "1119/1119 [==============================] - 1s 920us/sample - loss: 1.7106 - acc: 0.2913 - val_loss: 1.7663 - val_acc: 0.2464\n",
      "Epoch 73/100\n",
      "1119/1119 [==============================] - 1s 889us/sample - loss: 1.6868 - acc: 0.3092 - val_loss: 1.7745 - val_acc: 0.2393\n",
      "Epoch 74/100\n",
      "1119/1119 [==============================] - 1s 935us/sample - loss: 1.6913 - acc: 0.2913 - val_loss: 1.7674 - val_acc: 0.2429\n",
      "Epoch 75/100\n",
      "1119/1119 [==============================] - 1s 897us/sample - loss: 1.6940 - acc: 0.2976 - val_loss: 1.7705 - val_acc: 0.2536\n",
      "Epoch 76/100\n",
      "1119/1119 [==============================] - 1s 855us/sample - loss: 1.6925 - acc: 0.2869 - val_loss: 1.7573 - val_acc: 0.2464\n",
      "Epoch 77/100\n",
      "1119/1119 [==============================] - 1s 846us/sample - loss: 1.7002 - acc: 0.3146 - val_loss: 1.7632 - val_acc: 0.2500\n",
      "Epoch 78/100\n",
      "1119/1119 [==============================] - 1s 897us/sample - loss: 1.6847 - acc: 0.3083 - val_loss: 1.7674 - val_acc: 0.2429\n",
      "Epoch 79/100\n",
      "1119/1119 [==============================] - 1s 941us/sample - loss: 1.6924 - acc: 0.2940 - val_loss: 1.7745 - val_acc: 0.2429\n",
      "Epoch 80/100\n",
      "1119/1119 [==============================] - 1s 849us/sample - loss: 1.6944 - acc: 0.2752 - val_loss: 1.7771 - val_acc: 0.2393\n",
      "Epoch 81/100\n",
      "1119/1119 [==============================] - 1s 877us/sample - loss: 1.6764 - acc: 0.3092 - val_loss: 1.7820 - val_acc: 0.2143\n",
      "Epoch 82/100\n",
      "1119/1119 [==============================] - 1s 838us/sample - loss: 1.6774 - acc: 0.3172 - val_loss: 1.7758 - val_acc: 0.2286\n",
      "Epoch 83/100\n",
      "1119/1119 [==============================] - 1s 866us/sample - loss: 1.7092 - acc: 0.2967 - val_loss: 1.7758 - val_acc: 0.2357\n",
      "Epoch 84/100\n",
      "1119/1119 [==============================] - 1s 880us/sample - loss: 1.6768 - acc: 0.2958 - val_loss: 1.7826 - val_acc: 0.2393\n",
      "Epoch 85/100\n",
      "1119/1119 [==============================] - 1s 881us/sample - loss: 1.6857 - acc: 0.3038 - val_loss: 1.7780 - val_acc: 0.2643\n",
      "Epoch 86/100\n",
      "1119/1119 [==============================] - 1s 858us/sample - loss: 1.7150 - acc: 0.2806 - val_loss: 1.7708 - val_acc: 0.2321\n",
      "Epoch 87/100\n",
      "1119/1119 [==============================] - 1s 869us/sample - loss: 1.7090 - acc: 0.2797 - val_loss: 1.7704 - val_acc: 0.2536\n",
      "Epoch 88/100\n",
      "1119/1119 [==============================] - 1s 838us/sample - loss: 1.6990 - acc: 0.2833 - val_loss: 1.7712 - val_acc: 0.2393\n",
      "Epoch 89/100\n",
      "1119/1119 [==============================] - 1s 839us/sample - loss: 1.6894 - acc: 0.2895 - val_loss: 1.7704 - val_acc: 0.2286\n",
      "Epoch 90/100\n",
      "1119/1119 [==============================] - 1s 898us/sample - loss: 1.6934 - acc: 0.2913 - val_loss: 1.7699 - val_acc: 0.2536\n",
      "Epoch 91/100\n",
      "1119/1119 [==============================] - 1s 890us/sample - loss: 1.7058 - acc: 0.2940 - val_loss: 1.7652 - val_acc: 0.2536\n",
      "Epoch 92/100\n",
      "1119/1119 [==============================] - 1s 905us/sample - loss: 1.6975 - acc: 0.2887 - val_loss: 1.7678 - val_acc: 0.2321\n",
      "Epoch 93/100\n",
      "1119/1119 [==============================] - 1s 905us/sample - loss: 1.6837 - acc: 0.2904 - val_loss: 1.7726 - val_acc: 0.2429\n",
      "Epoch 94/100\n",
      "1119/1119 [==============================] - 1s 894us/sample - loss: 1.7068 - acc: 0.2895 - val_loss: 1.7767 - val_acc: 0.2464\n",
      "Epoch 95/100\n",
      "1119/1119 [==============================] - 1s 884us/sample - loss: 1.6981 - acc: 0.2976 - val_loss: 1.7731 - val_acc: 0.2286\n",
      "Epoch 96/100\n",
      "1119/1119 [==============================] - 1s 911us/sample - loss: 1.6996 - acc: 0.2833 - val_loss: 1.7716 - val_acc: 0.2357\n",
      "Epoch 97/100\n",
      "1119/1119 [==============================] - 1s 922us/sample - loss: 1.7137 - acc: 0.2940 - val_loss: 1.7739 - val_acc: 0.2429\n",
      "Epoch 98/100\n",
      "1119/1119 [==============================] - 1s 904us/sample - loss: 1.7024 - acc: 0.2904 - val_loss: 1.7767 - val_acc: 0.2393\n",
      "Epoch 99/100\n",
      "1119/1119 [==============================] - 1s 898us/sample - loss: 1.6740 - acc: 0.2967 - val_loss: 1.7823 - val_acc: 0.1964\n",
      "Epoch 100/100\n",
      "1119/1119 [==============================] - 1s 893us/sample - loss: 1.6768 - acc: 0.2949 - val_loss: 1.7889 - val_acc: 0.2179\n"
     ]
    }
   ],
   "source": [
    "emotion_vm.model.train(X_savee_train, Y_savee_train, batch_size=10, validation_data=(X_savee_test, Y_savee_test), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[57  0  0  0  1 16]\n",
      " [30  0  1  0  0  7]\n",
      " [37  0  1  1  0  5]\n",
      " [34  0  0  0  0 10]\n",
      " [25  0  1  0  0 12]\n",
      " [32  0  1  0  1  8]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.27      0.77      0.39        74\n",
      "       happy       0.00      0.00      0.00        38\n",
      "       angry       0.25      0.02      0.04        44\n",
      "     fearful       0.00      0.00      0.00        44\n",
      "   surprised       0.00      0.00      0.00        38\n",
      "         sad       0.14      0.19      0.16        42\n",
      "\n",
      "    accuracy                           0.24       280\n",
      "   macro avg       0.11      0.16      0.10       280\n",
      "weighted avg       0.13      0.24      0.13       280\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_metrics(X_savee_test, Y_savee_test, emotion_list, emotion_vm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
