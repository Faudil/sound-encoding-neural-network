{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# Import phase if every import is here it is to allow you to run this cell and know if you have any issue and not after a few hours of training\n",
    "\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, Activation, Flatten, MaxPooling1D, BatchNormalization, LSTM, GaussianNoise\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from src.VoiceModule import VoiceModule\n",
    "from src.classifiers.KerasClassifier import KerasClassifier\n",
    "from prepare_data_utils import load_wav, preprare_wav\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fearful\n",
      "savee\n",
      "calm\n",
      "happy\n",
      "bdes\n",
      "surprised\n",
      "angry\n",
      "sad\n",
      "keywords\n"
     ]
    }
   ],
   "source": [
    "# print environment\n",
    "folder_path = \"../../data\"\n",
    "for l in os.listdir(folder_path):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data method\n",
    "\n",
    "def mfcc(buffer, samplerate, dim):\n",
    "    a = librosa.feature.mfcc(y=buffer, sr=samplerate, n_mfcc=dim)\n",
    "    a = np.transpose(a)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callback to later draw some graphs\n",
    "\n",
    "class PerformanceLogger(Callback):\n",
    "    def __init__(self):\n",
    "        # List of tuple (loss, accuracy)\n",
    "        self.training = []\n",
    "        self.testing = []\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.training.append((logs[\"loss\"], logs[\"acc\"]))\n",
    "        self.testing.append((logs[\"val_loss\"], logs[\"val_acc\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function prints metrics about a model\n",
    "def print_metrics(voice_module, X, Y, label_name_list):\n",
    "    Y_pred = voice_module.model._model.predict(X)\n",
    "    Y_pred = np.argmax(Y_pred, axis=1)\n",
    "    Y_max = np.argmax(Y, axis=1)\n",
    "    print(confusion_matrix(Y_max, Y_pred))\n",
    "    print(classification_report(Y_max, Y_pred, target_names=label_name_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class\n",
    "class EmotionClassifierCnn(KerasClassifier):\n",
    "    def __init__(self, file_path=None):\n",
    "        super().__init__(file_path)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = np.expand_dims(np.array([x]), axis=2)\n",
    "        return self._model.predict(x)\n",
    "\n",
    "    def build(self):\n",
    "        model = Sequential()\n",
    "        model.add(GaussianNoise(0.4))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv1D(256, 3, input_shape=(128,13), padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=8))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Conv1D(128, 3, padding='same'))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(GaussianNoise(0.4))\n",
    "        model.add(Dense(64))\n",
    "        model.add(Dense(5))\n",
    "        model.add(Activation('softmax'))\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        self._model = model\n",
    "        \n",
    "    def train(self, X, Y, batch_size=32, epoch=720, validation_data=None, callbacks=[]):\n",
    "        self._model.fit(X, Y, batch_size=batch_size, epochs=epoch, validation_data=validation_data, callbacks=callbacks)\n",
    "\n",
    "    def transform(self, x, samplerate):\n",
    "        to_process = mfcc(x, samplerate, 13)\n",
    "        to_process = pad_sequences([to_process], maxlen=128, padding='post')[0]\n",
    "        return to_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-9787e4c96440>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmotionClassifierCnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVoiceModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"emotion\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1502\u001b[0m     \"\"\"\n\u001b[1;32m   1503\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1504\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   1505\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1506\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."
     ]
    }
   ],
   "source": [
    "#Instanciate model\n",
    "emotion_list = [\"calm\", \"happy\", \"angry\", \"fearful\", \"sad\"]\n",
    "dimension_mfcc = 13\n",
    "samplerate = 16000\n",
    "nb_break=None\n",
    "sample_duration=3\n",
    "step=2\n",
    "\n",
    "logger = PerformanceLogger()\n",
    "\n",
    "cls = EmotionClassifierCnn()\n",
    "vm = VoiceModule(\"emotion\", emotion_list, cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for training\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X, Y = preprare_wav(data, vm, sample_duration, step)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_file = f\"x_{'_'.join(emotion_list)}-{vm._name}.npy\"\n",
    "Y_file = f\"y_{'_'.join(emotion_list)}-{vm._name}.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the prepared input data \n",
    "np.save(X_file, X)\n",
    "np.save(Y_file, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for training\n",
    "X, Y = np.load(X_file), np.load(Y_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769 193\n",
      "(128, 13)\n",
      "float32\n"
     ]
    }
   ],
   "source": [
    "\"\"\"train_size = 80 * len(X) // 100\n",
    "test_size = 20 * len(X) // 100\n",
    "#X = np.expand_dims(X, axis=3)\n",
    "X_train, X_test, = X[:train_size], X[train_size:]\n",
    "Y_train, Y_test = Y[:train_size], Y[train_size:]\"\"\"\n",
    "X = X.astype('float32')\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "print(len(X_train), len(X_test))\n",
    "\n",
    "print(X[0].shape)\n",
    "print( X.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769 samples, validate on 193 samples\n",
      "Epoch 1/100\n",
      "769/769 [==============================] - 2s 2ms/sample - loss: 2.1659 - acc: 0.3888 - val_loss: 2.6906 - val_acc: 0.3782\n",
      "Epoch 2/100\n",
      "769/769 [==============================] - 0s 649us/sample - loss: 1.3094 - acc: 0.5657 - val_loss: 5.2212 - val_acc: 0.2383\n",
      "Epoch 3/100\n",
      "769/769 [==============================] - 0s 627us/sample - loss: 0.9738 - acc: 0.6359 - val_loss: 4.7832 - val_acc: 0.2332\n",
      "Epoch 4/100\n",
      "769/769 [==============================] - 1s 655us/sample - loss: 0.8012 - acc: 0.6827 - val_loss: 4.9264 - val_acc: 0.2332\n",
      "Epoch 5/100\n",
      "769/769 [==============================] - 0s 605us/sample - loss: 0.7181 - acc: 0.7178 - val_loss: 3.7580 - val_acc: 0.2332\n",
      "Epoch 6/100\n",
      "769/769 [==============================] - 0s 623us/sample - loss: 0.6452 - acc: 0.7633 - val_loss: 2.6569 - val_acc: 0.2798\n",
      "Epoch 7/100\n",
      "769/769 [==============================] - 0s 583us/sample - loss: 0.5544 - acc: 0.7893 - val_loss: 1.9019 - val_acc: 0.3731\n",
      "Epoch 8/100\n",
      "769/769 [==============================] - 0s 581us/sample - loss: 0.4942 - acc: 0.8153 - val_loss: 2.0991 - val_acc: 0.3316\n",
      "Epoch 9/100\n",
      "769/769 [==============================] - 0s 564us/sample - loss: 0.4530 - acc: 0.8153 - val_loss: 3.1468 - val_acc: 0.2539\n",
      "Epoch 10/100\n",
      "769/769 [==============================] - 0s 550us/sample - loss: 0.4437 - acc: 0.8518 - val_loss: 2.0715 - val_acc: 0.3575\n",
      "Epoch 11/100\n",
      "769/769 [==============================] - 0s 628us/sample - loss: 0.4030 - acc: 0.8596 - val_loss: 1.6715 - val_acc: 0.4663\n",
      "Epoch 12/100\n",
      "769/769 [==============================] - 0s 634us/sample - loss: 0.3438 - acc: 0.8622 - val_loss: 1.3280 - val_acc: 0.5440\n",
      "Epoch 13/100\n",
      "769/769 [==============================] - 0s 603us/sample - loss: 0.2965 - acc: 0.9064 - val_loss: 0.9726 - val_acc: 0.6373\n",
      "Epoch 14/100\n",
      "769/769 [==============================] - 1s 676us/sample - loss: 0.3039 - acc: 0.8882 - val_loss: 0.8826 - val_acc: 0.6684\n",
      "Epoch 15/100\n",
      "769/769 [==============================] - 1s 744us/sample - loss: 0.2776 - acc: 0.8960 - val_loss: 1.0598 - val_acc: 0.6218\n",
      "Epoch 16/100\n",
      "769/769 [==============================] - 1s 711us/sample - loss: 0.2955 - acc: 0.8869 - val_loss: 0.7333 - val_acc: 0.7461\n",
      "Epoch 17/100\n",
      "769/769 [==============================] - 1s 764us/sample - loss: 0.3090 - acc: 0.8895 - val_loss: 0.6443 - val_acc: 0.7668\n",
      "Epoch 18/100\n",
      "769/769 [==============================] - 1s 654us/sample - loss: 0.2801 - acc: 0.8947 - val_loss: 0.6755 - val_acc: 0.7668\n",
      "Epoch 19/100\n",
      "769/769 [==============================] - 0s 546us/sample - loss: 0.2559 - acc: 0.9051 - val_loss: 0.8231 - val_acc: 0.7358\n",
      "Epoch 20/100\n",
      "769/769 [==============================] - 1s 702us/sample - loss: 0.2749 - acc: 0.8973 - val_loss: 0.7004 - val_acc: 0.7772\n",
      "Epoch 21/100\n",
      "769/769 [==============================] - 0s 527us/sample - loss: 0.2143 - acc: 0.9129 - val_loss: 0.5805 - val_acc: 0.7979\n",
      "Epoch 22/100\n",
      "769/769 [==============================] - 0s 638us/sample - loss: 0.2294 - acc: 0.9168 - val_loss: 0.4551 - val_acc: 0.8342\n",
      "Epoch 23/100\n",
      "769/769 [==============================] - 0s 621us/sample - loss: 0.2216 - acc: 0.9259 - val_loss: 0.5800 - val_acc: 0.8083\n",
      "Epoch 24/100\n",
      "769/769 [==============================] - 0s 615us/sample - loss: 0.2218 - acc: 0.9233 - val_loss: 0.5587 - val_acc: 0.8135\n",
      "Epoch 25/100\n",
      "769/769 [==============================] - 1s 653us/sample - loss: 0.1839 - acc: 0.9428 - val_loss: 0.6047 - val_acc: 0.7927\n",
      "Epoch 26/100\n",
      "769/769 [==============================] - 1s 747us/sample - loss: 0.1744 - acc: 0.9441 - val_loss: 0.7351 - val_acc: 0.7668\n",
      "Epoch 27/100\n",
      "769/769 [==============================] - 1s 739us/sample - loss: 0.1955 - acc: 0.9324 - val_loss: 0.6856 - val_acc: 0.7720\n",
      "Epoch 28/100\n",
      "769/769 [==============================] - 0s 632us/sample - loss: 0.1706 - acc: 0.9350 - val_loss: 0.5550 - val_acc: 0.8135\n",
      "Epoch 29/100\n",
      "769/769 [==============================] - 1s 759us/sample - loss: 0.1663 - acc: 0.9454 - val_loss: 0.5026 - val_acc: 0.8342\n",
      "Epoch 30/100\n",
      "769/769 [==============================] - 0s 622us/sample - loss: 0.1405 - acc: 0.9493 - val_loss: 0.5216 - val_acc: 0.8446\n",
      "Epoch 31/100\n",
      "769/769 [==============================] - 0s 583us/sample - loss: 0.1508 - acc: 0.9415 - val_loss: 0.4985 - val_acc: 0.8342\n",
      "Epoch 32/100\n",
      "769/769 [==============================] - 0s 643us/sample - loss: 0.2218 - acc: 0.9194 - val_loss: 0.5841 - val_acc: 0.8601\n",
      "Epoch 33/100\n",
      "769/769 [==============================] - 0s 566us/sample - loss: 0.2019 - acc: 0.9337 - val_loss: 0.5058 - val_acc: 0.8238\n",
      "Epoch 34/100\n",
      "769/769 [==============================] - 0s 648us/sample - loss: 0.1455 - acc: 0.9467 - val_loss: 0.5119 - val_acc: 0.8756\n",
      "Epoch 35/100\n",
      "769/769 [==============================] - 0s 629us/sample - loss: 0.1538 - acc: 0.9428 - val_loss: 0.7112 - val_acc: 0.7720\n",
      "Epoch 36/100\n",
      "769/769 [==============================] - 0s 579us/sample - loss: 0.1601 - acc: 0.9480 - val_loss: 0.5742 - val_acc: 0.8290\n",
      "Epoch 37/100\n",
      "769/769 [==============================] - 0s 620us/sample - loss: 0.2118 - acc: 0.9285 - val_loss: 0.7176 - val_acc: 0.7876\n",
      "Epoch 38/100\n",
      "769/769 [==============================] - 0s 554us/sample - loss: 0.1962 - acc: 0.9363 - val_loss: 0.6614 - val_acc: 0.8238\n",
      "Epoch 39/100\n",
      "769/769 [==============================] - 0s 616us/sample - loss: 0.1538 - acc: 0.9467 - val_loss: 0.5469 - val_acc: 0.8394\n",
      "Epoch 40/100\n",
      "769/769 [==============================] - 1s 659us/sample - loss: 0.1362 - acc: 0.9506 - val_loss: 0.4877 - val_acc: 0.8653\n",
      "Epoch 41/100\n",
      "769/769 [==============================] - 1s 695us/sample - loss: 0.1305 - acc: 0.9545 - val_loss: 0.4770 - val_acc: 0.8601\n",
      "Epoch 42/100\n",
      "769/769 [==============================] - 1s 652us/sample - loss: 0.0783 - acc: 0.9701 - val_loss: 0.5832 - val_acc: 0.8394\n",
      "Epoch 43/100\n",
      "769/769 [==============================] - 1s 665us/sample - loss: 0.1342 - acc: 0.9519 - val_loss: 0.7168 - val_acc: 0.8083\n",
      "Epoch 44/100\n",
      "769/769 [==============================] - 1s 700us/sample - loss: 0.1489 - acc: 0.9415 - val_loss: 0.8770 - val_acc: 0.7668\n",
      "Epoch 45/100\n",
      "769/769 [==============================] - 1s 718us/sample - loss: 0.0904 - acc: 0.9688 - val_loss: 0.6699 - val_acc: 0.7876\n",
      "Epoch 46/100\n",
      "769/769 [==============================] - 0s 587us/sample - loss: 0.1429 - acc: 0.9428 - val_loss: 0.7747 - val_acc: 0.7876\n",
      "Epoch 47/100\n",
      "769/769 [==============================] - 0s 603us/sample - loss: 0.0928 - acc: 0.9649 - val_loss: 0.6253 - val_acc: 0.8135\n",
      "Epoch 48/100\n",
      "769/769 [==============================] - 0s 646us/sample - loss: 0.1181 - acc: 0.9610 - val_loss: 0.6215 - val_acc: 0.8342\n",
      "Epoch 49/100\n",
      "769/769 [==============================] - 0s 634us/sample - loss: 0.1141 - acc: 0.9623 - val_loss: 0.6826 - val_acc: 0.8135\n",
      "Epoch 50/100\n",
      "769/769 [==============================] - 0s 524us/sample - loss: 0.1423 - acc: 0.9467 - val_loss: 0.9641 - val_acc: 0.7617\n",
      "Epoch 51/100\n",
      "769/769 [==============================] - 0s 584us/sample - loss: 0.1749 - acc: 0.9402 - val_loss: 0.9259 - val_acc: 0.7720\n",
      "Epoch 52/100\n",
      "769/769 [==============================] - 1s 683us/sample - loss: 0.1647 - acc: 0.9441 - val_loss: 0.6872 - val_acc: 0.8187\n",
      "Epoch 53/100\n",
      "769/769 [==============================] - 1s 659us/sample - loss: 0.1297 - acc: 0.9532 - val_loss: 0.8886 - val_acc: 0.7927\n",
      "Epoch 54/100\n",
      "769/769 [==============================] - 0s 562us/sample - loss: 0.1265 - acc: 0.9532 - val_loss: 1.1813 - val_acc: 0.7461\n",
      "Epoch 55/100\n",
      "769/769 [==============================] - 0s 586us/sample - loss: 0.1308 - acc: 0.9558 - val_loss: 1.5445 - val_acc: 0.7047\n",
      "Epoch 56/100\n",
      "769/769 [==============================] - 1s 668us/sample - loss: 0.1254 - acc: 0.9597 - val_loss: 1.1148 - val_acc: 0.7461\n",
      "Epoch 57/100\n",
      "769/769 [==============================] - 0s 643us/sample - loss: 0.1221 - acc: 0.9571 - val_loss: 0.7933 - val_acc: 0.8238\n",
      "Epoch 58/100\n",
      "769/769 [==============================] - 1s 666us/sample - loss: 0.1209 - acc: 0.9636 - val_loss: 0.7879 - val_acc: 0.7927\n",
      "Epoch 59/100\n",
      "769/769 [==============================] - 1s 651us/sample - loss: 0.1230 - acc: 0.9597 - val_loss: 0.5835 - val_acc: 0.8497\n",
      "Epoch 60/100\n",
      "769/769 [==============================] - 1s 711us/sample - loss: 0.0854 - acc: 0.9688 - val_loss: 0.8342 - val_acc: 0.7617\n",
      "Epoch 61/100\n",
      "769/769 [==============================] - 1s 712us/sample - loss: 0.0660 - acc: 0.9753 - val_loss: 0.9146 - val_acc: 0.7824\n",
      "Epoch 62/100\n",
      "769/769 [==============================] - 1s 696us/sample - loss: 0.1043 - acc: 0.9662 - val_loss: 0.9831 - val_acc: 0.7668\n",
      "Epoch 63/100\n",
      "769/769 [==============================] - 0s 629us/sample - loss: 0.1098 - acc: 0.9623 - val_loss: 0.9565 - val_acc: 0.8187\n",
      "Epoch 64/100\n",
      "769/769 [==============================] - 0s 620us/sample - loss: 0.1108 - acc: 0.9649 - val_loss: 0.7350 - val_acc: 0.8031\n",
      "Epoch 65/100\n",
      "769/769 [==============================] - 1s 695us/sample - loss: 0.1274 - acc: 0.9623 - val_loss: 0.7616 - val_acc: 0.7927\n",
      "Epoch 66/100\n",
      "769/769 [==============================] - 1s 679us/sample - loss: 0.1128 - acc: 0.9610 - val_loss: 0.7306 - val_acc: 0.8446\n",
      "Epoch 67/100\n",
      "769/769 [==============================] - 0s 639us/sample - loss: 0.0677 - acc: 0.9766 - val_loss: 1.0786 - val_acc: 0.8083\n",
      "Epoch 68/100\n",
      "769/769 [==============================] - 1s 789us/sample - loss: 0.1275 - acc: 0.9506 - val_loss: 0.7939 - val_acc: 0.8083\n",
      "Epoch 69/100\n",
      "769/769 [==============================] - 1s 705us/sample - loss: 0.0837 - acc: 0.9727 - val_loss: 0.9392 - val_acc: 0.8031\n",
      "Epoch 70/100\n",
      "769/769 [==============================] - 1s 701us/sample - loss: 0.0625 - acc: 0.9818 - val_loss: 0.6911 - val_acc: 0.8394\n",
      "Epoch 71/100\n",
      "769/769 [==============================] - 1s 670us/sample - loss: 0.0827 - acc: 0.9675 - val_loss: 0.7582 - val_acc: 0.8238\n",
      "Epoch 72/100\n",
      "769/769 [==============================] - 1s 668us/sample - loss: 0.0945 - acc: 0.9675 - val_loss: 0.7859 - val_acc: 0.8083\n",
      "Epoch 73/100\n",
      "769/769 [==============================] - 1s 708us/sample - loss: 0.0932 - acc: 0.9714 - val_loss: 0.7479 - val_acc: 0.8083\n",
      "Epoch 74/100\n",
      "769/769 [==============================] - 1s 717us/sample - loss: 0.0819 - acc: 0.9727 - val_loss: 0.5914 - val_acc: 0.8497\n",
      "Epoch 75/100\n",
      "769/769 [==============================] - 0s 629us/sample - loss: 0.1036 - acc: 0.9701 - val_loss: 0.5693 - val_acc: 0.8290\n",
      "Epoch 76/100\n",
      "769/769 [==============================] - 0s 587us/sample - loss: 0.0648 - acc: 0.9792 - val_loss: 0.7384 - val_acc: 0.7927\n",
      "Epoch 77/100\n",
      "769/769 [==============================] - 0s 650us/sample - loss: 0.0922 - acc: 0.9688 - val_loss: 0.7079 - val_acc: 0.8290\n",
      "Epoch 78/100\n",
      "769/769 [==============================] - 0s 591us/sample - loss: 0.0849 - acc: 0.9649 - val_loss: 0.7387 - val_acc: 0.8238\n",
      "Epoch 79/100\n",
      "769/769 [==============================] - 0s 593us/sample - loss: 0.0904 - acc: 0.9740 - val_loss: 0.6518 - val_acc: 0.8290\n",
      "Epoch 80/100\n",
      "769/769 [==============================] - 0s 507us/sample - loss: 0.0786 - acc: 0.9688 - val_loss: 0.6648 - val_acc: 0.8549\n",
      "Epoch 81/100\n",
      "769/769 [==============================] - 0s 504us/sample - loss: 0.0874 - acc: 0.9740 - val_loss: 0.6245 - val_acc: 0.8497\n",
      "Epoch 82/100\n",
      "769/769 [==============================] - 0s 538us/sample - loss: 0.0845 - acc: 0.9701 - val_loss: 0.7978 - val_acc: 0.8394\n",
      "Epoch 83/100\n",
      "769/769 [==============================] - 0s 513us/sample - loss: 0.0572 - acc: 0.9779 - val_loss: 0.6211 - val_acc: 0.8342\n",
      "Epoch 84/100\n",
      "769/769 [==============================] - 0s 620us/sample - loss: 0.1129 - acc: 0.9675 - val_loss: 0.6131 - val_acc: 0.8394\n",
      "Epoch 85/100\n",
      "769/769 [==============================] - 0s 561us/sample - loss: 0.0764 - acc: 0.9688 - val_loss: 0.7267 - val_acc: 0.8497\n",
      "Epoch 86/100\n",
      "769/769 [==============================] - 0s 589us/sample - loss: 0.1100 - acc: 0.9649 - val_loss: 0.7906 - val_acc: 0.8290\n",
      "Epoch 87/100\n",
      "769/769 [==============================] - 1s 652us/sample - loss: 0.0695 - acc: 0.9740 - val_loss: 0.8462 - val_acc: 0.8031\n",
      "Epoch 88/100\n",
      "769/769 [==============================] - 1s 668us/sample - loss: 0.0821 - acc: 0.9714 - val_loss: 0.6815 - val_acc: 0.8497\n",
      "Epoch 89/100\n",
      "769/769 [==============================] - 1s 669us/sample - loss: 0.0706 - acc: 0.9727 - val_loss: 0.6226 - val_acc: 0.8653\n",
      "Epoch 90/100\n",
      "769/769 [==============================] - 1s 654us/sample - loss: 0.0885 - acc: 0.9675 - val_loss: 0.6030 - val_acc: 0.8756\n",
      "Epoch 91/100\n",
      "769/769 [==============================] - 0s 621us/sample - loss: 0.0793 - acc: 0.9636 - val_loss: 0.7137 - val_acc: 0.8549\n",
      "Epoch 92/100\n",
      "769/769 [==============================] - 0s 597us/sample - loss: 0.0855 - acc: 0.9636 - val_loss: 0.9416 - val_acc: 0.8031\n",
      "Epoch 93/100\n",
      "769/769 [==============================] - 0s 637us/sample - loss: 0.1075 - acc: 0.9688 - val_loss: 0.9007 - val_acc: 0.8187\n",
      "Epoch 94/100\n",
      "769/769 [==============================] - 0s 616us/sample - loss: 0.0623 - acc: 0.9805 - val_loss: 0.8366 - val_acc: 0.8497\n",
      "Epoch 95/100\n",
      "769/769 [==============================] - 0s 593us/sample - loss: 0.1165 - acc: 0.9571 - val_loss: 0.8883 - val_acc: 0.8083\n",
      "Epoch 96/100\n",
      "769/769 [==============================] - 0s 623us/sample - loss: 0.0598 - acc: 0.9779 - val_loss: 0.7107 - val_acc: 0.8290\n",
      "Epoch 97/100\n",
      "769/769 [==============================] - 1s 668us/sample - loss: 0.0660 - acc: 0.9792 - val_loss: 0.7236 - val_acc: 0.8290\n",
      "Epoch 98/100\n",
      "769/769 [==============================] - 1s 682us/sample - loss: 0.0707 - acc: 0.9675 - val_loss: 0.8447 - val_acc: 0.7927\n",
      "Epoch 99/100\n",
      "769/769 [==============================] - 1s 671us/sample - loss: 0.0684 - acc: 0.9805 - val_loss: 0.7564 - val_acc: 0.8446\n",
      "Epoch 100/100\n",
      "769/769 [==============================] - 0s 648us/sample - loss: 0.0457 - acc: 0.9844 - val_loss: 0.7045 - val_acc: 0.8446\n"
     ]
    }
   ],
   "source": [
    "vm.model.train(X_train, Y_train, batch_size=64, epoch=100, validation_data=(X_test, Y_test), callbacks=[logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb02c0df208>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27  2  0  0  2]\n",
      " [ 0 34  6  3  2]\n",
      " [ 1  0 36  1  3]\n",
      " [ 0  2  0 34  3]\n",
      " [ 3  1  0  1 32]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.87      0.87      0.87        31\n",
      "       happy       0.87      0.76      0.81        45\n",
      "       angry       0.86      0.88      0.87        41\n",
      "     fearful       0.87      0.87      0.87        39\n",
      "         sad       0.76      0.86      0.81        37\n",
      "\n",
      "    accuracy                           0.84       193\n",
      "   macro avg       0.85      0.85      0.85       193\n",
      "weighted avg       0.85      0.84      0.84       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "plt.plot(list(range(0, len(logger.training))), list(map(lambda x:x[1], logger.training)), label=\"training data\")\n",
    "plt.plot(list(range(0, len(logger.testing))), list(map(lambda x:x[1], logger.testing)), label=\"testing data\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.show()\n",
    "print_metrics(vm, X_test, Y_test, emotion_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this graph we can see that the model largely overfit around the 20st epoch\n",
    "We can also see that the testing data's accuracy continues to grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we save the model so we don't have to retrain it each time\n",
    "vm.model.save(\"emotion_cnn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "gaussian_noise (GaussianNois multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  52        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  10240     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  98432     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "gaussian_noise_1 (GaussianNo multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  65600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  325       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    multiple                  0         \n",
      "=================================================================\n",
      "Total params: 178,745\n",
      "Trainable params: 176,671\n",
      "Non-trainable params: 2,074\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Now we save the model so we don't have to retrain it each time\n",
    "vm.model.load(\"emotion_cnn.model\")\n",
    "vm.model._model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n"
     ]
    }
   ],
   "source": [
    "# Now we load the new dataset (savee)\n",
    "\n",
    "# First we change the folder path\n",
    "folder_path = \"../../data/savee\"\n",
    "\n",
    "# Now we load the data\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X_savee, Y_savee = preprare_wav(data, vm, sample_duration, step)\n",
    "X_savee = X_savee.astype('float32')\n",
    "X_savee_train, X_savee_test, Y_savee_train, Y_savee_test = train_test_split(X_savee, Y_savee, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"X_savee_{emotion_list}-{vm._name}\", X_savee)\n",
    "np.save(f\"Y_savee_{emotion_list}-{vm._name}\", Y_savee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360/360 [==============================] - 0s 390us/sample - loss: 9.4428 - acc: 0.2667\n",
      "[[25 14  3  2 26]\n",
      " [10 24  7  9 22]\n",
      " [ 6 21 13  9 19]\n",
      " [15  8 11 12 19]\n",
      " [42 12  8  1 22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.26      0.36      0.30        70\n",
      "       happy       0.30      0.33      0.32        72\n",
      "       angry       0.31      0.19      0.24        68\n",
      "     fearful       0.36      0.18      0.24        65\n",
      "         sad       0.20      0.26      0.23        85\n",
      "\n",
      "    accuracy                           0.27       360\n",
      "   macro avg       0.29      0.27      0.26       360\n",
      "weighted avg       0.28      0.27      0.26       360\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm.model._model.evaluate(X_savee, Y_savee)\n",
    "print_metrics(vm, X_savee, Y_savee, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/arc/fdp5/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "gaussian_noise\n",
      "batch_normalization\n",
      "conv1d\n",
      "activation\n",
      "max_pooling1d\n",
      "dropout\n",
      "conv1d_1\n",
      "activation_1\n",
      "max_pooling1d_1\n",
      "flatten\n",
      "batch_normalization_1\n",
      "dropout_1\n",
      "gaussian_noise_1\n"
     ]
    }
   ],
   "source": [
    "# Now we freeze every layer used for extracting features from data\n",
    "savee_model = keras.models.clone_model(vm.model._model)\n",
    "for layer in savee_model.layers[:-3]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "savee_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "savee_cls = EmotionClassifierCnn()\n",
    "savee_cls._model = savee_model\n",
    "savee_vm = VoiceModule(\"emotion\", emotion_list, savee_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 288 samples, validate on 72 samples\n",
      "Epoch 1/100\n",
      "288/288 [==============================] - 1s 4ms/sample - loss: 2.5873 - acc: 0.2847 - val_loss: 137.9696 - val_acc: 0.2222\n",
      "Epoch 2/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 1.6694 - acc: 0.4757 - val_loss: 122.9169 - val_acc: 0.2222\n",
      "Epoch 3/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 1.2866 - acc: 0.5938 - val_loss: 135.1570 - val_acc: 0.2222\n",
      "Epoch 4/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 1.0873 - acc: 0.6146 - val_loss: 155.7931 - val_acc: 0.2222\n",
      "Epoch 5/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 1.1846 - acc: 0.6458 - val_loss: 184.3281 - val_acc: 0.2222\n",
      "Epoch 6/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.8503 - acc: 0.6736 - val_loss: 166.2670 - val_acc: 0.2222\n",
      "Epoch 7/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.9845 - acc: 0.6458 - val_loss: 194.8858 - val_acc: 0.2222\n",
      "Epoch 8/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.8178 - acc: 0.7049 - val_loss: 177.2951 - val_acc: 0.2222\n",
      "Epoch 9/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7492 - acc: 0.7431 - val_loss: 212.2648 - val_acc: 0.2222\n",
      "Epoch 10/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.8882 - acc: 0.6701 - val_loss: 219.1042 - val_acc: 0.2222\n",
      "Epoch 11/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7129 - acc: 0.7743 - val_loss: 237.4382 - val_acc: 0.2222\n",
      "Epoch 12/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7597 - acc: 0.7465 - val_loss: 210.0345 - val_acc: 0.2222\n",
      "Epoch 13/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.8757 - acc: 0.7188 - val_loss: 229.0569 - val_acc: 0.2222\n",
      "Epoch 14/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.8050 - acc: 0.7083 - val_loss: 242.8238 - val_acc: 0.2222\n",
      "Epoch 15/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5943 - acc: 0.7951 - val_loss: 234.0798 - val_acc: 0.2222\n",
      "Epoch 16/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6325 - acc: 0.7986 - val_loss: 235.8093 - val_acc: 0.2222\n",
      "Epoch 17/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6765 - acc: 0.7847 - val_loss: 230.7889 - val_acc: 0.2222\n",
      "Epoch 18/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.8050 - acc: 0.7326 - val_loss: 239.1712 - val_acc: 0.2222\n",
      "Epoch 19/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7009 - acc: 0.7326 - val_loss: 222.5048 - val_acc: 0.2222\n",
      "Epoch 20/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6389 - acc: 0.7778 - val_loss: 239.4620 - val_acc: 0.2222\n",
      "Epoch 21/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7904 - acc: 0.7639 - val_loss: 210.0296 - val_acc: 0.2222\n",
      "Epoch 22/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7434 - acc: 0.7535 - val_loss: 246.2515 - val_acc: 0.2222\n",
      "Epoch 23/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6406 - acc: 0.7674 - val_loss: 226.3649 - val_acc: 0.2222\n",
      "Epoch 24/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5515 - acc: 0.7986 - val_loss: 215.3666 - val_acc: 0.2222\n",
      "Epoch 25/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6513 - acc: 0.7639 - val_loss: 230.2212 - val_acc: 0.2222\n",
      "Epoch 26/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6870 - acc: 0.7535 - val_loss: 215.6598 - val_acc: 0.2222\n",
      "Epoch 27/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6214 - acc: 0.7604 - val_loss: 228.7014 - val_acc: 0.2222\n",
      "Epoch 28/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6114 - acc: 0.7986 - val_loss: 252.4540 - val_acc: 0.2222\n",
      "Epoch 29/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6291 - acc: 0.7812 - val_loss: 253.9318 - val_acc: 0.2222\n",
      "Epoch 30/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6418 - acc: 0.7778 - val_loss: 205.3657 - val_acc: 0.2222\n",
      "Epoch 31/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.7436 - acc: 0.7569 - val_loss: 228.0679 - val_acc: 0.2222\n",
      "Epoch 32/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6428 - acc: 0.7917 - val_loss: 236.7191 - val_acc: 0.2222\n",
      "Epoch 33/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6239 - acc: 0.7847 - val_loss: 227.0516 - val_acc: 0.2222\n",
      "Epoch 34/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.7059 - acc: 0.7396 - val_loss: 202.9639 - val_acc: 0.2222\n",
      "Epoch 35/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.5508 - acc: 0.8056 - val_loss: 206.3347 - val_acc: 0.2222\n",
      "Epoch 36/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.6054 - acc: 0.7882 - val_loss: 179.7072 - val_acc: 0.2222\n",
      "Epoch 37/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6627 - acc: 0.7778 - val_loss: 200.8601 - val_acc: 0.2222\n",
      "Epoch 38/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6528 - acc: 0.7639 - val_loss: 203.8399 - val_acc: 0.2222\n",
      "Epoch 39/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5204 - acc: 0.8299 - val_loss: 188.2256 - val_acc: 0.2222\n",
      "Epoch 40/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5756 - acc: 0.7847 - val_loss: 157.8873 - val_acc: 0.2222\n",
      "Epoch 41/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5274 - acc: 0.7986 - val_loss: 173.4080 - val_acc: 0.2222\n",
      "Epoch 42/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5176 - acc: 0.8021 - val_loss: 166.0536 - val_acc: 0.2222\n",
      "Epoch 43/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4939 - acc: 0.8299 - val_loss: 183.5792 - val_acc: 0.2222\n",
      "Epoch 44/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6318 - acc: 0.7708 - val_loss: 209.3729 - val_acc: 0.2222\n",
      "Epoch 45/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4879 - acc: 0.7951 - val_loss: 188.8976 - val_acc: 0.2222\n",
      "Epoch 46/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6854 - acc: 0.7431 - val_loss: 179.7847 - val_acc: 0.2222\n",
      "Epoch 47/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5150 - acc: 0.8299 - val_loss: 181.3135 - val_acc: 0.2222\n",
      "Epoch 48/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5314 - acc: 0.8021 - val_loss: 188.1326 - val_acc: 0.2222\n",
      "Epoch 49/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6829 - acc: 0.7743 - val_loss: 186.1980 - val_acc: 0.2222\n",
      "Epoch 50/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.5127 - acc: 0.8056 - val_loss: 198.9040 - val_acc: 0.2222\n",
      "Epoch 51/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.5376 - acc: 0.8125 - val_loss: 201.0662 - val_acc: 0.2222\n",
      "Epoch 52/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5539 - acc: 0.7917 - val_loss: 190.7466 - val_acc: 0.2222\n",
      "Epoch 53/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5279 - acc: 0.7986 - val_loss: 186.9359 - val_acc: 0.2222\n",
      "Epoch 54/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4656 - acc: 0.8229 - val_loss: 173.4318 - val_acc: 0.2222\n",
      "Epoch 55/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5758 - acc: 0.7847 - val_loss: 180.6795 - val_acc: 0.2222\n",
      "Epoch 56/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5805 - acc: 0.7708 - val_loss: 178.4200 - val_acc: 0.2222\n",
      "Epoch 57/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6307 - acc: 0.7882 - val_loss: 190.5052 - val_acc: 0.2222\n",
      "Epoch 58/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4443 - acc: 0.8160 - val_loss: 205.4617 - val_acc: 0.2222\n",
      "Epoch 59/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4999 - acc: 0.8021 - val_loss: 196.9496 - val_acc: 0.2222\n",
      "Epoch 60/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5812 - acc: 0.7743 - val_loss: 167.7459 - val_acc: 0.2222\n",
      "Epoch 61/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4388 - acc: 0.8194 - val_loss: 169.8005 - val_acc: 0.2222\n",
      "Epoch 62/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6335 - acc: 0.7882 - val_loss: 185.1731 - val_acc: 0.2222\n",
      "Epoch 63/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5660 - acc: 0.7882 - val_loss: 184.1876 - val_acc: 0.2222\n",
      "Epoch 64/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5736 - acc: 0.7917 - val_loss: 165.6359 - val_acc: 0.2222\n",
      "Epoch 65/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6043 - acc: 0.8021 - val_loss: 183.3739 - val_acc: 0.2222\n",
      "Epoch 66/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4221 - acc: 0.8438 - val_loss: 193.0966 - val_acc: 0.2222\n",
      "Epoch 67/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5189 - acc: 0.7951 - val_loss: 212.0829 - val_acc: 0.2222\n",
      "Epoch 68/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4315 - acc: 0.8368 - val_loss: 218.0284 - val_acc: 0.2222\n",
      "Epoch 69/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5287 - acc: 0.8229 - val_loss: 204.8233 - val_acc: 0.2222\n",
      "Epoch 70/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4621 - acc: 0.8090 - val_loss: 192.6683 - val_acc: 0.2222\n",
      "Epoch 71/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.6109 - acc: 0.7674 - val_loss: 190.9682 - val_acc: 0.2222\n",
      "Epoch 72/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5170 - acc: 0.8021 - val_loss: 195.0471 - val_acc: 0.2222\n",
      "Epoch 73/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4486 - acc: 0.8264 - val_loss: 197.1278 - val_acc: 0.2222\n",
      "Epoch 74/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5346 - acc: 0.8090 - val_loss: 192.8943 - val_acc: 0.2222\n",
      "Epoch 75/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5097 - acc: 0.8160 - val_loss: 205.4934 - val_acc: 0.2222\n",
      "Epoch 76/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.4622 - acc: 0.8125 - val_loss: 193.7549 - val_acc: 0.2222\n",
      "Epoch 77/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4845 - acc: 0.8090 - val_loss: 221.7631 - val_acc: 0.2222\n",
      "Epoch 78/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4355 - acc: 0.8542 - val_loss: 198.5062 - val_acc: 0.2222\n",
      "Epoch 79/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.3852 - acc: 0.8507 - val_loss: 199.2508 - val_acc: 0.2222\n",
      "Epoch 80/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4659 - acc: 0.8229 - val_loss: 198.5417 - val_acc: 0.2222\n",
      "Epoch 81/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5019 - acc: 0.8056 - val_loss: 195.4448 - val_acc: 0.2222\n",
      "Epoch 82/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.3762 - acc: 0.8542 - val_loss: 184.6067 - val_acc: 0.2222\n",
      "Epoch 83/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4571 - acc: 0.8090 - val_loss: 176.3392 - val_acc: 0.2222\n",
      "Epoch 84/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5215 - acc: 0.7951 - val_loss: 184.1510 - val_acc: 0.2222\n",
      "Epoch 85/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4456 - acc: 0.8542 - val_loss: 185.3776 - val_acc: 0.2222\n",
      "Epoch 86/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5009 - acc: 0.8056 - val_loss: 165.5589 - val_acc: 0.2222\n",
      "Epoch 87/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4913 - acc: 0.8403 - val_loss: 157.2239 - val_acc: 0.2222\n",
      "Epoch 88/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5007 - acc: 0.8229 - val_loss: 195.0132 - val_acc: 0.2222\n",
      "Epoch 89/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4645 - acc: 0.7882 - val_loss: 202.4750 - val_acc: 0.2222\n",
      "Epoch 90/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5995 - acc: 0.7708 - val_loss: 195.6350 - val_acc: 0.2222\n",
      "Epoch 91/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4584 - acc: 0.8160 - val_loss: 206.6252 - val_acc: 0.2222\n",
      "Epoch 92/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.6173 - acc: 0.7778 - val_loss: 219.7964 - val_acc: 0.2222\n",
      "Epoch 93/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4691 - acc: 0.8160 - val_loss: 204.7656 - val_acc: 0.2222\n",
      "Epoch 94/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4627 - acc: 0.8090 - val_loss: 181.6638 - val_acc: 0.2222\n",
      "Epoch 95/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.4903 - acc: 0.7951 - val_loss: 176.6993 - val_acc: 0.2222\n",
      "Epoch 96/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.4828 - acc: 0.8368 - val_loss: 191.9827 - val_acc: 0.2222\n",
      "Epoch 97/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.4466 - acc: 0.8125 - val_loss: 201.2991 - val_acc: 0.2222\n",
      "Epoch 98/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5313 - acc: 0.8160 - val_loss: 182.0542 - val_acc: 0.2222\n",
      "Epoch 99/100\n",
      "288/288 [==============================] - 0s 2ms/sample - loss: 0.4342 - acc: 0.8160 - val_loss: 193.7540 - val_acc: 0.2222\n",
      "Epoch 100/100\n",
      "288/288 [==============================] - 0s 1ms/sample - loss: 0.5049 - acc: 0.8125 - val_loss: 193.0365 - val_acc: 0.2222\n"
     ]
    }
   ],
   "source": [
    "# And we train the two last layers\n",
    "# This is classic fine tuning\n",
    "savee_vm.model.train(X_savee_train, Y_savee_train, batch_size=10, validation_data=(X_savee_test, Y_savee_test), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16  0  0  0  0]\n",
      " [18  0  0  0  0]\n",
      " [ 9  0  0  0  0]\n",
      " [10  0  0  0  0]\n",
      " [19  0  0  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.22      1.00      0.36        16\n",
      "       happy       0.00      0.00      0.00        18\n",
      "       angry       0.00      0.00      0.00         9\n",
      "     fearful       0.00      0.00      0.00        10\n",
      "         sad       0.00      0.00      0.00        19\n",
      "\n",
      "    accuracy                           0.22        72\n",
      "   macro avg       0.04      0.20      0.07        72\n",
      "weighted avg       0.05      0.22      0.08        72\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_metrics(savee_vm, X_savee_test, Y_savee_test, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing calm\n",
      "Doing happy\n",
      "Doing angry\n",
      "Doing fearful\n",
      "Doing sad\n"
     ]
    }
   ],
   "source": [
    "# Now we load the new dataset (savee)\n",
    "\n",
    "# First we change the folder path\n",
    "folder_path = \"../../data/bdes\"\n",
    "\n",
    "# Now we load the data\n",
    "data = {f: load_wav(f\"{folder_path}/{f}\", nb_break) for f in emotion_list}\n",
    "X_bdes, Y_bdes = preprare_wav(data, vm, sample_duration, step)\n",
    "X_bdes = X_bdes.astype('float32')\n",
    "X_bdes_train, X_bdes_test, Y_bdes_train, Y_bdes_test = train_test_split(X_bdes, Y_bdes, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f\"X_bdes_{vm._name}\", X_bdes)\n",
    "np.save(f\"Y_bdes_{vm._name}\", Y_bdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388/388 [==============================] - 0s 293us/sample - loss: 10.0543 - acc: 0.2552\n",
      "[[16  1  7 51  4]\n",
      " [ 2  5  3 60  1]\n",
      " [ 0  4 10 73  1]\n",
      " [ 4  0  1 64  0]\n",
      " [54  0  3 20  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.21      0.20      0.21        79\n",
      "       happy       0.50      0.07      0.12        71\n",
      "       angry       0.42      0.11      0.18        88\n",
      "     fearful       0.24      0.93      0.38        69\n",
      "         sad       0.40      0.05      0.09        81\n",
      "\n",
      "    accuracy                           0.26       388\n",
      "   macro avg       0.35      0.27      0.20       388\n",
      "weighted avg       0.35      0.26      0.19       388\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vm.model._model.evaluate(X_bdes, Y_bdes)\n",
    "print_metrics(vm, X_bdes, Y_bdes, emotion_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gaussian_noise\n",
      "batch_normalization\n",
      "conv1d\n",
      "activation\n",
      "max_pooling1d\n",
      "dropout\n",
      "conv1d_1\n",
      "activation_1\n",
      "max_pooling1d_1\n",
      "flatten\n",
      "batch_normalization_1\n",
      "dropout_1\n",
      "gaussian_noise_1\n"
     ]
    }
   ],
   "source": [
    "# Now we freeze every layer used for extracting features from data\n",
    "bdes_model = keras.models.clone_model(vm.model._model)\n",
    "for layer in bdes_model.layers[:-3]:\n",
    "    print(layer.name)\n",
    "    layer.trainable = False\n",
    "bdes_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "bdes_cls = EmotionClassifierCnn()\n",
    "bdes_cls._model = bdes_model\n",
    "bdes_vm = VoiceModule(\"emotion\", emotion_list, bdes_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 310 samples, validate on 78 samples\n",
      "Epoch 1/100\n",
      "310/310 [==============================] - 1s 4ms/sample - loss: 2.2885 - acc: 0.3806 - val_loss: 64.2890 - val_acc: 0.2179\n",
      "Epoch 2/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 1.2393 - acc: 0.5742 - val_loss: 87.0077 - val_acc: 0.2308\n",
      "Epoch 3/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.9029 - acc: 0.6968 - val_loss: 105.3604 - val_acc: 0.1795\n",
      "Epoch 4/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 1.0171 - acc: 0.6516 - val_loss: 94.3597 - val_acc: 0.2308\n",
      "Epoch 5/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 1.0496 - acc: 0.6710 - val_loss: 87.9391 - val_acc: 0.1667\n",
      "Epoch 6/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7118 - acc: 0.7677 - val_loss: 99.0741 - val_acc: 0.1795\n",
      "Epoch 7/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7547 - acc: 0.7677 - val_loss: 101.4368 - val_acc: 0.2051\n",
      "Epoch 8/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7024 - acc: 0.7387 - val_loss: 109.8000 - val_acc: 0.2308\n",
      "Epoch 9/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7345 - acc: 0.7452 - val_loss: 113.4250 - val_acc: 0.2051\n",
      "Epoch 10/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7166 - acc: 0.7742 - val_loss: 115.6071 - val_acc: 0.2179\n",
      "Epoch 11/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.6618 - acc: 0.7677 - val_loss: 112.7831 - val_acc: 0.1795\n",
      "Epoch 12/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.7109 - acc: 0.7903 - val_loss: 117.6982 - val_acc: 0.1795\n",
      "Epoch 13/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5763 - acc: 0.8065 - val_loss: 125.5366 - val_acc: 0.1795\n",
      "Epoch 14/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5671 - acc: 0.8194 - val_loss: 125.9607 - val_acc: 0.1667\n",
      "Epoch 15/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5559 - acc: 0.7935 - val_loss: 126.3437 - val_acc: 0.1795\n",
      "Epoch 16/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.6727 - acc: 0.7935 - val_loss: 122.7736 - val_acc: 0.1923\n",
      "Epoch 17/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5457 - acc: 0.7774 - val_loss: 126.4173 - val_acc: 0.1923\n",
      "Epoch 18/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.6072 - acc: 0.7774 - val_loss: 147.6798 - val_acc: 0.1795\n",
      "Epoch 19/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5255 - acc: 0.8161 - val_loss: 164.3750 - val_acc: 0.1923\n",
      "Epoch 20/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4980 - acc: 0.8194 - val_loss: 151.1572 - val_acc: 0.1923\n",
      "Epoch 21/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4825 - acc: 0.8129 - val_loss: 124.8211 - val_acc: 0.1538\n",
      "Epoch 22/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.6314 - acc: 0.7839 - val_loss: 123.6752 - val_acc: 0.1538\n",
      "Epoch 23/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4469 - acc: 0.8548 - val_loss: 127.0466 - val_acc: 0.1538\n",
      "Epoch 24/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4945 - acc: 0.8355 - val_loss: 128.1699 - val_acc: 0.1538\n",
      "Epoch 25/100\n",
      "310/310 [==============================] - 0s 2ms/sample - loss: 0.5381 - acc: 0.8258 - val_loss: 114.1082 - val_acc: 0.2308\n",
      "Epoch 26/100\n",
      "310/310 [==============================] - 0s 2ms/sample - loss: 0.5548 - acc: 0.8226 - val_loss: 119.4117 - val_acc: 0.1795\n",
      "Epoch 27/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5375 - acc: 0.7871 - val_loss: 119.4393 - val_acc: 0.2051\n",
      "Epoch 28/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5216 - acc: 0.8032 - val_loss: 123.5461 - val_acc: 0.1667\n",
      "Epoch 29/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5508 - acc: 0.8161 - val_loss: 129.9421 - val_acc: 0.1538\n",
      "Epoch 30/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4345 - acc: 0.8677 - val_loss: 124.3021 - val_acc: 0.1538\n",
      "Epoch 31/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4174 - acc: 0.8645 - val_loss: 125.9432 - val_acc: 0.1410\n",
      "Epoch 32/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4655 - acc: 0.8355 - val_loss: 145.4288 - val_acc: 0.1282\n",
      "Epoch 33/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4428 - acc: 0.8387 - val_loss: 145.4580 - val_acc: 0.1538\n",
      "Epoch 34/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3772 - acc: 0.8710 - val_loss: 142.6235 - val_acc: 0.1538\n",
      "Epoch 35/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5260 - acc: 0.8452 - val_loss: 144.3913 - val_acc: 0.1667\n",
      "Epoch 36/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4499 - acc: 0.8258 - val_loss: 143.4331 - val_acc: 0.1538\n",
      "Epoch 37/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5294 - acc: 0.8258 - val_loss: 144.3860 - val_acc: 0.1667\n",
      "Epoch 38/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3921 - acc: 0.8806 - val_loss: 126.0021 - val_acc: 0.1538\n",
      "Epoch 39/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4699 - acc: 0.8290 - val_loss: 108.2425 - val_acc: 0.2564\n",
      "Epoch 40/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4919 - acc: 0.8290 - val_loss: 124.0890 - val_acc: 0.1923\n",
      "Epoch 41/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3944 - acc: 0.8419 - val_loss: 144.9167 - val_acc: 0.1923\n",
      "Epoch 42/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3639 - acc: 0.8806 - val_loss: 124.7209 - val_acc: 0.1795\n",
      "Epoch 43/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4645 - acc: 0.8387 - val_loss: 122.6875 - val_acc: 0.1667\n",
      "Epoch 44/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4595 - acc: 0.8581 - val_loss: 117.7809 - val_acc: 0.1667\n",
      "Epoch 45/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4690 - acc: 0.8194 - val_loss: 111.1898 - val_acc: 0.1410\n",
      "Epoch 46/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4635 - acc: 0.8226 - val_loss: 128.8851 - val_acc: 0.1538\n",
      "Epoch 47/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3081 - acc: 0.8903 - val_loss: 142.4160 - val_acc: 0.1795\n",
      "Epoch 48/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4537 - acc: 0.8290 - val_loss: 124.7520 - val_acc: 0.2051\n",
      "Epoch 49/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4783 - acc: 0.8226 - val_loss: 131.3058 - val_acc: 0.1923\n",
      "Epoch 50/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.5028 - acc: 0.8645 - val_loss: 118.7258 - val_acc: 0.1282\n",
      "Epoch 51/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4247 - acc: 0.8290 - val_loss: 105.1799 - val_acc: 0.1282\n",
      "Epoch 52/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3655 - acc: 0.8677 - val_loss: 103.7534 - val_acc: 0.2051\n",
      "Epoch 53/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4400 - acc: 0.8323 - val_loss: 108.7790 - val_acc: 0.2179\n",
      "Epoch 54/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4889 - acc: 0.8323 - val_loss: 107.5755 - val_acc: 0.2179\n",
      "Epoch 55/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4357 - acc: 0.8419 - val_loss: 118.8459 - val_acc: 0.2308\n",
      "Epoch 56/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3479 - acc: 0.8774 - val_loss: 129.5714 - val_acc: 0.1795\n",
      "Epoch 57/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3986 - acc: 0.8419 - val_loss: 133.9625 - val_acc: 0.1923\n",
      "Epoch 58/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3214 - acc: 0.8710 - val_loss: 133.8387 - val_acc: 0.2051\n",
      "Epoch 59/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3432 - acc: 0.8710 - val_loss: 130.7243 - val_acc: 0.1923\n",
      "Epoch 60/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3286 - acc: 0.8935 - val_loss: 126.2687 - val_acc: 0.1923\n",
      "Epoch 61/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3624 - acc: 0.8613 - val_loss: 119.8979 - val_acc: 0.1795\n",
      "Epoch 62/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4851 - acc: 0.8226 - val_loss: 92.5696 - val_acc: 0.2564\n",
      "Epoch 63/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3412 - acc: 0.8839 - val_loss: 90.1317 - val_acc: 0.2692\n",
      "Epoch 64/100\n",
      "310/310 [==============================] - 0s 2ms/sample - loss: 0.4055 - acc: 0.8258 - val_loss: 107.1997 - val_acc: 0.1667\n",
      "Epoch 65/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4852 - acc: 0.8097 - val_loss: 111.4990 - val_acc: 0.1795\n",
      "Epoch 66/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3592 - acc: 0.8645 - val_loss: 114.6545 - val_acc: 0.1923\n",
      "Epoch 67/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3625 - acc: 0.8484 - val_loss: 95.2640 - val_acc: 0.1667\n",
      "Epoch 68/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4989 - acc: 0.8355 - val_loss: 97.4889 - val_acc: 0.1538\n",
      "Epoch 69/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4567 - acc: 0.8290 - val_loss: 88.5685 - val_acc: 0.1795\n",
      "Epoch 70/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4049 - acc: 0.8710 - val_loss: 86.1048 - val_acc: 0.2051\n",
      "Epoch 71/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4279 - acc: 0.8355 - val_loss: 95.8450 - val_acc: 0.1923\n",
      "Epoch 72/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4176 - acc: 0.8613 - val_loss: 83.2581 - val_acc: 0.2821\n",
      "Epoch 73/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3718 - acc: 0.8323 - val_loss: 99.8688 - val_acc: 0.1795\n",
      "Epoch 74/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4112 - acc: 0.8419 - val_loss: 100.0877 - val_acc: 0.1923\n",
      "Epoch 75/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.2494 - acc: 0.9032 - val_loss: 107.6611 - val_acc: 0.1410\n",
      "Epoch 76/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3780 - acc: 0.8516 - val_loss: 105.4418 - val_acc: 0.1410\n",
      "Epoch 77/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3625 - acc: 0.8677 - val_loss: 105.1547 - val_acc: 0.1795\n",
      "Epoch 78/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4356 - acc: 0.8323 - val_loss: 122.8271 - val_acc: 0.1923\n",
      "Epoch 79/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4107 - acc: 0.8452 - val_loss: 112.9184 - val_acc: 0.1923\n",
      "Epoch 80/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4108 - acc: 0.8613 - val_loss: 109.2217 - val_acc: 0.1795\n",
      "Epoch 81/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3192 - acc: 0.8839 - val_loss: 94.3908 - val_acc: 0.1410\n",
      "Epoch 82/100\n",
      "310/310 [==============================] - 0s 2ms/sample - loss: 0.4241 - acc: 0.8419 - val_loss: 106.9575 - val_acc: 0.1667\n",
      "Epoch 83/100\n",
      "310/310 [==============================] - 0s 2ms/sample - loss: 0.3940 - acc: 0.8613 - val_loss: 92.8081 - val_acc: 0.1923\n",
      "Epoch 84/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3622 - acc: 0.8645 - val_loss: 89.9255 - val_acc: 0.1795\n",
      "Epoch 85/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.2302 - acc: 0.9032 - val_loss: 98.9029 - val_acc: 0.1667\n",
      "Epoch 86/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3320 - acc: 0.8774 - val_loss: 92.8756 - val_acc: 0.1667\n",
      "Epoch 87/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3994 - acc: 0.8774 - val_loss: 86.1952 - val_acc: 0.1282\n",
      "Epoch 88/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3438 - acc: 0.8742 - val_loss: 81.0756 - val_acc: 0.1410\n",
      "Epoch 89/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.2950 - acc: 0.8806 - val_loss: 90.4444 - val_acc: 0.2051\n",
      "Epoch 90/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4321 - acc: 0.8516 - val_loss: 73.8203 - val_acc: 0.2436\n",
      "Epoch 91/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3601 - acc: 0.8645 - val_loss: 95.5542 - val_acc: 0.2051\n",
      "Epoch 92/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3514 - acc: 0.8581 - val_loss: 84.7307 - val_acc: 0.2308\n",
      "Epoch 93/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4221 - acc: 0.8355 - val_loss: 91.3170 - val_acc: 0.2051\n",
      "Epoch 94/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4462 - acc: 0.8323 - val_loss: 72.6223 - val_acc: 0.2436\n",
      "Epoch 95/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3727 - acc: 0.8581 - val_loss: 76.5176 - val_acc: 0.2436\n",
      "Epoch 96/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3477 - acc: 0.8742 - val_loss: 87.1698 - val_acc: 0.2308\n",
      "Epoch 97/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3158 - acc: 0.8581 - val_loss: 84.8716 - val_acc: 0.1667\n",
      "Epoch 98/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.4032 - acc: 0.8548 - val_loss: 99.7495 - val_acc: 0.1795\n",
      "Epoch 99/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3407 - acc: 0.8935 - val_loss: 109.7359 - val_acc: 0.1795\n",
      "Epoch 100/100\n",
      "310/310 [==============================] - 0s 1ms/sample - loss: 0.3952 - acc: 0.8548 - val_loss: 98.6922 - val_acc: 0.1667\n"
     ]
    }
   ],
   "source": [
    "# And we train the two last layers\n",
    "# This is classic fine tuning\n",
    "bdes_vm.model.train(X_bdes_train, Y_bdes_train, batch_size=10, validation_data=(X_bdes_test, Y_bdes_test), epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  9 14  0  0]\n",
      " [ 0  5 10  0  0]\n",
      " [ 0  7  8  0  0]\n",
      " [ 0  2  8  0  0]\n",
      " [ 0 10  5  0  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        calm       0.00      0.00      0.00        23\n",
      "       happy       0.15      0.33      0.21        15\n",
      "       angry       0.18      0.53      0.27        15\n",
      "     fearful       0.00      0.00      0.00        10\n",
      "         sad       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.17        78\n",
      "   macro avg       0.07      0.17      0.10        78\n",
      "weighted avg       0.06      0.17      0.09        78\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arc/fdp5/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print_metrics(bdes_vm, X_bdes_test, Y_bdes_test, emotion_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
